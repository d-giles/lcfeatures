{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This notebook is being used to optimize features and relies on files not included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featurecalc as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_fl = '/home/dgiles/Documents/KeplerLCs/filelists/Q1_filelist_completed.txt'\n",
    "with open(Q1_fl,'r') as file:\n",
    "    f0 = file.readline()[:-1]\n",
    "\n",
    "path_to_fits = '/home/dgiles/Documents/KeplerLCs/fitsFiles/Q1fitsfiles/'\n",
    "testfile = path_to_fits+f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dgiles/Documents/KeplerLCs/fitsFiles/Q1fitsfiles/'"
      ]
     },
     "execution_count": 999,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not numpy.bytes_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-997-2d5499bd6114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeplerml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_files_w_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ1_fl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_fits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/KeplerML/python/keplerml.py\u001b[0m in \u001b[0;36mfl_files_w_path\u001b[0;34m(fl, fitsDir)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# char.array allows appending the fits directory later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mfcreate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Creates a filelist to keep track of processed files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mfcreate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/numpy/core/defchararray.py\u001b[0m in \u001b[0;36m__radd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         \u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m         \"\"\"\n\u001b[0;32m-> 1948\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/numpy/core/defchararray.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_num_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_get_num_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_use_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__add__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not numpy.bytes_"
     ]
    }
   ],
   "source": [
    "files = fc.fl_files_w_path(Q1_fl,path_to_fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib as imp\n",
    "imp.reload(fc)\n",
    "nfile,ndata = fc.features_from_fits(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488 ms ± 32.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fc.features_from_fits(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         279376 function calls (277913 primitive calls) in 0.678 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "     6501    0.105    0.000    0.452    0.000 function_base.py:3517(_median)\n",
       "     6517    0.102    0.000    0.102    0.000 {method 'partition' of 'numpy.ndarray' objects}\n",
       "     7170    0.082    0.000    0.162    0.000 _methods.py:53(_mean)\n",
       "     7224    0.051    0.000    0.051    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "        1    0.026    0.026    0.275    0.275 keplerml.py:331(<listcomp>)\n",
       "     6517    0.025    0.000    0.486    0.000 function_base.py:3369(_ureduce)\n",
       "     7167    0.021    0.000    0.184    0.000 fromnumeric.py:2843(mean)\n",
       "    13034    0.019    0.000    0.029    0.000 numerictypes.py:728(issubdtype)\n",
       "    26929    0.017    0.000    0.017    0.000 {built-in method numpy.core.multiarray.array}\n",
       "     6517    0.017    0.000    0.017    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
       "     6581    0.012    0.000    0.012    0.000 numeric.py:1404(rollaxis)\n",
       "     6501    0.012    0.000    0.495    0.000 function_base.py:3429(median)\n",
       "     7179    0.011    0.000    0.014    0.000 _methods.py:43(_count_reduce_items)\n",
       "        1    0.010    0.010    0.615    0.615 keplerml.py:117(featureCalculation)\n",
       "     6501    0.010    0.000    0.133    0.000 fromnumeric.py:559(partition)\n",
       "    26747    0.009    0.000    0.025    0.000 numeric.py:484(asanyarray)\n",
       "    19157    0.007    0.000    0.007    0.000 {built-in method builtins.isinstance}\n",
       "    33501    0.006    0.000    0.006    0.000 {built-in method builtins.issubclass}\n",
       "     5364    0.006    0.000    0.006    0.000 {built-in method builtins.max}\n",
       "        1    0.006    0.006    0.006    0.006 keplerml.py:161(<listcomp>)\n",
       "     1090    0.006    0.000    0.006    0.000 {method 'match' of '_sre.SRE_Pattern' objects}\n",
       "        1    0.006    0.006    0.252    0.252 keplerml.py:347(<listcomp>)\n",
       "    13034    0.006    0.000    0.009    0.000 numerictypes.py:660(issubclass_)\n",
       "     5196    0.005    0.000    0.005    0.000 {built-in method builtins.min}\n",
       "        1    0.005    0.005    0.009    0.009 keplerml.py:359(<listcomp>)\n",
       "        1    0.005    0.005    0.009    0.009 keplerml.py:384(<listcomp>)\n",
       "        1    0.004    0.004    0.008    0.008 keplerml.py:371(<listcomp>)\n",
       "        1    0.004    0.004    0.008    0.008 keplerml.py:394(<listcomp>)\n",
       "     7574    0.003    0.000    0.004    0.000 {built-in method builtins.hasattr}\n",
       "     8440    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}\n",
       "      261    0.003    0.000    0.009    0.000 header.py:1026(append)\n",
       "        4    0.003    0.001    0.003    0.001 {built-in method numpy.linalg.lapack_lite.dgelsd}\n",
       "     6580    0.003    0.000    0.003    0.000 {built-in method builtins.abs}\n",
       "       19    0.002    0.000    0.002    0.000 inspect.py:2452(__init__)\n",
       "      166    0.002    0.000    0.012    0.000 card.py:708(_parse_value)\n",
       "        9    0.002    0.000    0.002    0.000 header.py:524(_find_end_card)\n",
       "2238/1658    0.002    0.000    0.002    0.000 {built-in method builtins.getattr}\n",
       "     7044    0.002    0.000    0.002    0.000 {method 'get' of 'dict' objects}\n",
       "      261    0.002    0.000    0.003    0.000 card.py:668(_parse_keyword)\n",
       "      427    0.001    0.000    0.003    0.000 card.py:622(_check_if_rvkc_image)\n",
       "        1    0.001    0.001    0.001    0.001 keplerml.py:214(<listcomp>)\n",
       "       60    0.001    0.000    0.001    0.000 {pandas._libs.lib.maybe_convert_objects}\n",
       "7082/7073    0.001    0.000    0.001    0.000 {built-in method builtins.len}\n",
       "        3    0.001    0.000    0.013    0.004 header.py:298(fromstring)\n",
       "        1    0.001    0.001    0.001    0.001 keplerml.py:164(<listcomp>)\n",
       "        1    0.001    0.001    0.020    0.020 column.py:1292(_init_from_table)\n",
       "       40    0.001    0.000    0.011    0.000 column.py:461(__init__)\n",
       "     1616    0.001    0.000    0.004    0.000 card.py:209(keyword)\n",
       "      400    0.001    0.000    0.006    0.000 column.py:430(__set__)\n",
       "      206    0.001    0.000    0.003    0.000 card.py:574(_check_if_rvkc)\n",
       "      301    0.001    0.000    0.003    0.000 card.py:151(__init__)\n",
       "      360    0.001    0.000    0.001    0.000 {built-in method builtins.locals}\n",
       "       16    0.001    0.000    0.002    0.000 function_base.py:3709(_percentile)\n",
       "      166    0.001    0.000    0.002    0.000 card.py:799(_split)\n",
       "      238    0.001    0.000    0.014    0.000 card.py:269(value)\n",
       "        1    0.001    0.001    0.001    0.001 keplerml.py:270(<listcomp>)\n",
       "  760/400    0.001    0.000    0.007    0.000 {built-in method builtins.setattr}\n",
       "  646/591    0.001    0.000    0.001    0.000 records.py:432(__getattribute__)\n",
       "        1    0.001    0.001    0.001    0.001 keplerml.py:267(<listcomp>)\n",
       "  654/591    0.001    0.000    0.002    0.000 column.py:1461(__getitem__)\n",
       "       29    0.001    0.000    0.003    0.000 column.py:1396(__getattr__)\n",
       "      685    0.001    0.000    0.001    0.000 {method 'find' of 'str' objects}\n",
       "     1009    0.001    0.000    0.001    0.000 {method 'group' of '_sre.SRE_Match' objects}\n",
       "      261    0.001    0.000    0.002    0.000 card.py:526(fromstring)\n",
       "      691    0.001    0.000    0.001    0.000 card.py:457(field_specifier)\n",
       "        3    0.001    0.000    0.001    0.000 {built-in method io.open}\n",
       "      310    0.001    0.000    0.002    0.000 card.py:540(normalize_keyword)\n",
       "      750    0.001    0.000    0.001    0.000 column.py:424(__get__)\n",
       "       63    0.001    0.000    0.001    0.000 util.py:1218(_str_to_num)\n",
       "   199/11    0.001    0.000    0.034    0.003 util.py:239(__get__)\n",
       "      637    0.001    0.000    0.001    0.000 util.py:1214(_is_int)\n",
       "        1    0.000    0.000    0.063    0.063 keplerml.py:55(read_kepler_curve)\n",
       "       11    0.000    0.000    0.000    0.000 {method 'read' of '_io.FileIO' objects}\n",
       "       94    0.000    0.000    0.000    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
       "      156    0.000    0.000    0.012    0.000 header.py:884(iteritems)\n",
       "        1    0.000    0.000    0.001    0.001 keplerml.py:156(<listcomp>)\n",
       "       40    0.000    0.000    0.001    0.000 column.py:1083(_convert_to_valid_data_type)\n",
       "        1    0.000    0.000    0.000    0.000 keplerml.py:176(<listcomp>)\n",
       "       40    0.000    0.000    0.001    0.000 card.py:289(value)\n",
       "   181/53    0.000    0.000    0.001    0.000 util.py:177(itersubclasses)\n",
       "       68    0.000    0.000    0.001    0.000 header.py:1518(_cardindex)\n",
       "        1    0.000    0.000    0.007    0.007 column.py:1259(_init_from_array)\n",
       "        1    0.000    0.000    0.000    0.000 keplerml.py:175(<listcomp>)\n",
       "        7    0.000    0.000    0.000    0.000 {method 'seek' of '_io.FileIO' objects}\n",
       "       43    0.000    0.000    0.003    0.000 header.py:112(__getitem__)\n",
       "        3    0.000    0.000    0.009    0.003 header.py:72(__init__)\n",
       "       41    0.000    0.000    0.000    0.000 weakref.py:102(__init__)\n",
       "       40    0.000    0.000    0.004    0.000 column.py:737(name)\n",
       "      713    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
       "       60    0.000    0.000    0.001    0.000 column.py:840(_verify_keywords)\n",
       "      629    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "        9    0.000    0.000    0.001    0.000 _methods.py:76(_var)\n",
       "       40    0.000    0.000    0.002    0.000 card.py:926(_format_image)\n",
       "        1    0.000    0.000    0.000    0.000 keplerml.py:292(<listcomp>)\n",
       "      166    0.000    0.000    0.000    0.000 {method 'union' of 'set' objects}\n",
       "       60    0.000    0.000    0.001    0.000 cast.py:971(maybe_cast_to_datetime)\n",
       "       38    0.000    0.000    0.001    0.000 records.py:466(__setattr__)\n",
       "       40    0.000    0.000    0.001    0.000 column.py:199(__new__)\n",
       "        3    0.000    0.000    0.003    0.001 inspect.py:2102(_signature_from_function)\n",
       "        6    0.000    0.000    0.002    0.000 base.py:51(_hdu_class_from_header)\n",
       "       60    0.000    0.000    0.001    0.000 column.py:697(array)\n",
       "       80    0.000    0.000    0.001    0.000 column.py:1990(_parse_tformat)\n",
       "      167    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "       40    0.000    0.000    0.001    0.000 card.py:890(_format_value)\n",
       "       63    0.000    0.000    0.001    0.000 py3compat.py:122(translate)\n",
       "       94    0.000    0.000    0.000    0.000 re.py:286(_compile)\n",
       "       63    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "      261    0.000    0.000    0.000    0.000 card.py:1242(_pad)\n",
       "        2    0.000    0.000    0.003    0.002 polynomial.py:402(polyfit)\n",
       "        2    0.000    0.000    0.002    0.001 image.py:38(__init__)\n",
       "      170    0.000    0.000    0.000    0.000 {method 'index' of 'str' objects}\n",
       "      427    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
       "        1    0.000    0.000    0.001    0.001 internals.py:4880(form_blocks)\n",
       "        4    0.000    0.000    0.024    0.006 base.py:411(_readfrom_internal)\n",
       "       60    0.000    0.000    0.001    0.000 column.py:1003(_determine_formats)\n",
       "        6    0.000    0.000    0.001    0.000 stats.py:888(_moment)\n",
       "        1    0.000    0.000    0.000    0.000 keplerml.py:203(<listcomp>)\n",
       "       42    0.000    0.000    0.001    0.000 util.py:91(_add_listener)\n",
       "      264    0.000    0.000    0.000    0.000 header.py:294(_modified)\n",
       "       53    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "        3    0.000    0.000    0.003    0.001 inspect.py:1089(getfullargspec)\n",
       "        2    0.000    0.000    0.003    0.002 linalg.py:1785(lstsq)\n",
       "       40    0.000    0.000    0.000    0.000 card.py:879(_format_keyword)\n",
       "       94    0.000    0.000    0.001    0.000 re.py:184(sub)\n",
       "      400    0.000    0.000    0.000    0.000 util.py:118(_notify)\n",
       "       60    0.000    0.000    0.000    0.000 series.py:4019(_sanitize_array)\n",
       "       35    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.concatenate}\n",
       "        1    0.000    0.000    0.003    0.003 column.py:1416(dtype)\n",
       "       23    0.000    0.000    0.001    0.000 records.py:544(field)\n",
       "       95    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x564616e2ee00}\n",
       "        1    0.000    0.000    0.678    0.678 keplerml.py:454(features_from_fits)\n",
       "       40    0.000    0.000    0.000    0.000 card.py:221(keyword)\n",
       "      315    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
       "        3    0.000    0.000    0.001    0.000 function_base.py:2298(cov)\n",
       "        4    0.000    0.000    0.015    0.004 header.py:428(_from_blocks)\n",
       "        1    0.000    0.000    0.002    0.002 table.py:180(_init_tbdata)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.writers.write_csv_rows}\n",
       "       19    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.001    0.000 fitsrec.py:610(_coldefs)\n",
       "       40    0.000    0.000    0.000    0.000 card.py:1173(_format_value)\n",
       "       11    0.000    0.000    0.000    0.000 abc.py:180(__instancecheck__)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:5017(_stack_arrays)\n",
       "      299    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "        4    0.000    0.000    0.024    0.006 base.py:338(readfrom)\n",
       "        9    0.000    0.000    0.000    0.000 stats.py:233(_contains_nan)\n",
       "       60    0.000    0.000    0.002    0.000 frame.py:7615(convert)\n",
       "        3    0.000    0.000    0.000    0.000 inspect.py:2732(__init__)\n",
       "      143    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:997(_handle_fromlist)\n",
       "       40    0.000    0.000    0.000    0.000 column.py:2143(_convert_fits2record)\n",
       "        1    0.000    0.000    0.000    0.000 memmap.py:201(__new__)\n",
       "       27    0.000    0.000    0.002    0.000 header.py:756(get)\n",
       "       20    0.000    0.000    0.000    0.000 column.py:2181(_convert_record2fits)\n",
       "       60    0.000    0.000    0.000    0.000 header.py:1679(_haswildcard)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:255(__new__)\n",
       "        2    0.000    0.000    0.028    0.014 column.py:1197(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:7644(_homogenize)\n",
       "       42    0.000    0.000    0.000    0.000 weakref.py:165(__setitem__)\n",
       "       23    0.000    0.000    0.000    0.000 {method 'getfield' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.dot}\n",
       "       13    0.000    0.000    0.000    0.000 {method 'tell' of '_io.FileIO' objects}\n",
       "        1    0.000    0.000    0.026    0.026 hdulist.py:774(_readfrom)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
       "       40    0.000    0.000    0.000    0.000 card.py:422(comment)\n",
       "        4    0.000    0.000    0.000    0.000 groups.py:274(match_header)\n",
       "       40    0.000    0.000    0.001    0.000 column.py:611(__hash__)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:870(_find_spec)\n",
       "    51/31    0.000    0.000    0.008    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.001    0.000 {pandas._libs.lib.clean_index_list}\n",
       "        1    0.000    0.000    0.001    0.001 file.py:457(_open_filename)\n",
       "        1    0.000    0.000    0.005    0.005 column.py:1335(<listcomp>)\n",
       "      200    0.000    0.000    0.000    0.000 generic.py:7(_check)\n",
       "        3    0.000    0.000    0.001    0.000 function_base.py:2500(corrcoef)\n",
       "       20    0.000    0.000    0.000    0.000 column.py:2225(_dtype_to_recformat)\n",
       "       40    0.000    0.000    0.002    0.000 card.py:492(image)\n",
       "       21    0.000    0.000    0.000    0.000 _weakrefset.py:70(__contains__)\n",
       "       41    0.000    0.000    0.000    0.000 weakref.py:288(update)\n",
       "       88    0.000    0.000    0.001    0.000 {method 'add' of 'set' objects}\n",
       "        1    0.000    0.000    0.008    0.008 fitsrec.py:219(__array_finalize__)\n",
       "       24    0.000    0.000    0.000    0.000 records.py:426(__array_finalize__)\n",
       "       61    0.000    0.000    0.000    0.000 base.py:2067(__getitem__)\n",
       "       20    0.000    0.000    0.000    0.000 numeric.py:2576(seterr)\n",
       "       61    0.000    0.000    0.000    0.000 internals.py:3148(get_block_type)\n",
       "       53    0.000    0.000    0.000    0.000 {method '__subclasses__' of 'type' objects}\n",
       "       67    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "       17    0.000    0.000    0.000    0.000 header.py:1866(__getitem__)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:146(__init__)\n",
       "        3    0.000    0.000    0.003    0.001 inspect.py:2183(_signature_from_callable)\n",
       "       61    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
       "        1    0.000    0.000    0.005    0.005 frame.py:334(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 twodim_base.py:490(vander)\n",
       "       36    0.000    0.000    0.000    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "       20    0.000    0.000    0.000    0.000 column.py:170(dtype)\n",
       "        1    0.000    0.000    0.678    0.678 {built-in method builtins.exec}\n",
       "       17    0.000    0.000    0.000    0.000 fromnumeric.py:1743(sum)\n",
       "       52    0.000    0.000    0.000    0.000 fitsrec.py:590(_coldefs)\n",
       "        2    0.000    0.000    0.001    0.001 stats.py:979(skew)\n",
       "      125    0.000    0.000    0.001    0.000 numeric.py:414(asarray)\n",
       "        3    0.000    0.000    0.001    0.000 fitsrec.py:677(field)\n",
       "       40    0.000    0.000    0.000    0.000 card.py:409(comment)\n",
       "        1    0.000    0.000    0.002    0.002 frame.py:7453(_to_arrays)\n",
       "    80/40    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
       "        3    0.000    0.000    0.001    0.000 base.py:927(size)\n",
       "      8/2    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:966(_find_and_load)\n",
       "        1    0.000    0.000    0.001    0.001 file.py:74(__init__)\n",
       "       20    0.000    0.000    0.000    0.000 numeric.py:2676(geterr)\n",
       "       60    0.000    0.000    0.001    0.000 column.py:2250(_convert_format)\n",
       "       40    0.000    0.000    0.000    0.000 column.py:228(canonical)\n",
       "       11    0.000    0.000    0.000    0.000 hdulist.py:172(__getitem__)\n",
       "       14    0.000    0.000    0.000    0.000 header.py:99(__contains__)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:28(__init__)\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.empty}\n",
       "       19    0.000    0.000    0.000    0.000 enum.py:267(__call__)\n",
       "       42    0.000    0.000    0.000    0.000 weakref.py:339(__init__)\n",
       "       40    0.000    0.000    0.001    0.000 _weakrefset.py:81(add)\n",
       "       20    0.000    0.000    0.001    0.000 column.py:216(from_recformat)\n",
       "       80    0.000    0.000    0.000    0.000 {method 'groups' of '_sre.SRE_Match' objects}\n",
       "        1    0.000    0.000    0.061    0.061 convenience.py:104(getdata)\n",
       "        3    0.000    0.000    0.000    0.000 function_base.py:857(average)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:288(_save_chunk)\n",
       "        4    0.000    0.000    0.000    0.000 hdulist.py:600(update_extend)\n",
       "       60    0.000    0.000    0.000    0.000 cast.py:853(maybe_castable)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "        3    0.000    0.000    0.000    0.000 hdulist.py:430(append)\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:1653(to_csv)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1233(find_spec)\n",
       "       40    0.000    0.000    0.002    0.000 card.py:200(__str__)\n",
       "       40    0.000    0.000    0.000    0.000 column.py:167(__hash__)\n",
       "        2    0.000    0.000    0.000    0.000 _util.py:26(_lazywhere)\n",
       "        9    0.000    0.000    0.000    0.000 {method 'finditer' of '_sre.SRE_Pattern' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'accumulate' of 'numpy.ufunc' objects}\n",
       "        1    0.000    0.000    0.034    0.034 table.py:147(_get_tbdata)\n",
       "        3    0.000    0.000    0.000    0.000 file.py:299(seek)\n",
       "       16    0.000    0.000    0.002    0.000 function_base.py:3588(percentile)\n",
       "       16    0.000    0.000    0.000    0.000 fromnumeric.py:1900(any)\n",
       "       17    0.000    0.000    0.000    0.000 header.py:251(cards)\n",
       "        1    0.000    0.000    0.000    0.000 fitsrec.py:617(_coldefs)\n",
       "        3    0.000    0.000    0.000    0.000 missing.py:189(_isna_ndarraylike)\n",
       "        6    0.000    0.000    0.001    0.000 _methods.py:122(_std)\n",
       "        4    0.000    0.000    0.015    0.004 header.py:368(fromfile)\n",
       "       20    0.000    0.000    0.000    0.000 column.py:728(array)\n",
       "        1    0.000    0.000    0.000    0.000 file.py:203(readarray)\n",
       "       40    0.000    0.000    0.000    0.000 column.py:222(recformat)\n",
       "        1    0.000    0.000    0.000    0.000 stats.py:1046(kurtosis)\n",
       "       40    0.000    0.000    0.000    0.000 card.py:920(_format_comment)\n",
       "       40    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
       "       60    0.000    0.000    0.000    0.000 internals.py:5020(_asarray_compat)\n",
       "       13    0.000    0.000    0.000    0.000 header.py:413(block_iter)\n",
       "        3    0.000    0.000    0.000    0.000 fitsrec.py:858(_convert_other)\n",
       "        1    0.000    0.000    0.000    0.000 _numpy_hacks.py:11(realign_dtype)\n",
       "        6    0.000    0.000    0.001    0.000 fromnumeric.py:2945(std)\n",
       "        1    0.000    0.000    0.026    0.026 convenience.py:664(_getext)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4986(_multi_blockify)\n",
       "       42    0.000    0.000    0.000    0.000 weakref.py:109(remove)\n",
       "        3    0.000    0.000    0.000    0.000 compressed.py:661(match_header)\n",
       "       20    0.000    0.000    0.000    0.000 column.py:134(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3363(_rebuild_blknos_and_blklocs)\n",
       "       42    0.000    0.000    0.000    0.000 weakref.py:334(__new__)\n",
       "      8/2    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:936(_find_and_load_unlocked)\n",
       "        2    0.000    0.000    0.000    0.000 _util.py:14(_valarray)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.copyto}\n",
       "        6    0.000    0.000    0.001    0.000 stats.py:817(moment)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.to_object_array_tuples}\n",
       "       12    0.000    0.000    0.000    0.000 fromnumeric.py:1383(ravel)\n",
       "        3    0.000    0.000    0.001    0.000 base.py:129(__new__)\n",
       "        3    0.000    0.000    0.003    0.001 py3compat.py:131(getargspec)\n",
       "        6    0.000    0.000    0.001    0.000 base.py:4914(_ensure_index)\n",
       "        4    0.000    0.000    0.000    0.000 nanfunctions.py:33(_replace_nan)\n",
       "       60    0.000    0.000    0.000    0.000 series.py:4036(_try_cast)\n",
       "       22    0.000    0.000    0.000    0.000 inspect.py:2781(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.missing.isnaobj}\n",
       "       41    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "       56    0.000    0.000    0.000    0.000 util.py:213(<lambda>)\n",
       "        1    0.000    0.000    0.002    0.002 frame.py:7547(_list_to_arrays)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "       10    0.000    0.000    0.000    0.000 file.py:190(read)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:121(_stringify_path)\n",
       "        1    0.000    0.000    0.002    0.002 frame.py:7621(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {method '__reduce__' of 'numpy.dtype' objects}\n",
       "       17    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 stats.py:1030(<lambda>)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:473(_simple_new)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:42(_wrapit)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:2825(round_)\n",
       "       40    0.000    0.000    0.000    0.000 _weakrefset.py:38(_remove)\n",
       "        2    0.000    0.000    0.000    0.000 table.py:701(match_header)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:861(_find_spec_legacy)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "       17    0.000    0.000    0.000    0.000 header.py:1836(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 image.py:924(match_header)\n",
       "        9    0.000    0.000    0.000    0.000 py3compat.py:29(decode_ascii)\n",
       "        1    0.000    0.000    0.000    0.000 numeric.py:607(require)\n",
       "        9    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}\n",
       "       26    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
       "        1    0.000    0.000    0.021    0.021 table.py:387(columns)\n",
       "        1    0.000    0.000    0.001    0.001 image.py:870(__init__)\n",
       "       61    0.000    0.000    0.000    0.000 common.py:122(is_sparse)\n",
       "       10    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.zeros}\n",
       "        1    0.000    0.000    0.000    0.000 table.py:245(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3265(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 dtypes.py:707(is_dtype)\n",
       "       64    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_scalar}\n",
       "        2    0.000    0.000    0.000    0.000 stride_tricks.py:137(broadcast_arrays)\n",
       "       36    0.000    0.000    0.000    0.000 fromnumeric.py:55(take)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:3062(var)\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method numpy.core.umath.seterrobj}\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:116(__init__)\n",
       "       63    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        2    0.000    0.000    0.000    0.000 _apipkg.py:110(__makeattr)\n",
       "        1    0.000    0.000    0.001    0.001 csvs.py:123(save)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:219(expand_dims)\n",
       "        4    0.000    0.000    0.000    0.000 function_base.py:1912(extract)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray._fastCopyAndTranspose}\n",
       "        1    0.000    0.000    0.026    0.026 hdulist.py:19(fitsopen)\n",
       "        4    0.000    0.000    0.000    0.000 nanfunctions.py:448(nansum)\n",
       "        9    0.000    0.000    0.000    0.000 numeric.py:2972(__exit__)\n",
       "       35    0.000    0.000    0.000    0.000 {method 'squeeze' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1117(_get_spec)\n",
       "       42    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        1    0.000    0.000    0.000    0.000 hdulist.py:133(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:909(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 header.py:92(__len__)\n",
       "       20    0.000    0.000    0.000    0.000 util.py:1181(_convert_array)\n",
       "        9    0.000    0.000    0.000    0.000 stats.py:201(_chk_asarray)\n",
       "        1    0.000    0.000    0.001    0.001 internals.py:4869(create_block_manager_from_arrays)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:124(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:130(lmap)\n",
       "        2    0.000    0.000    0.000    0.000 stride_tricks.py:118(_broadcast_shape)\n",
       "        9    0.000    0.000    0.000    0.000 numeric.py:2963(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 fitsrec.py:727(_cache_field)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2434(_format_native_types)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:301(_asarray_tuplesafe)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_dtype}\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:61(atleast_2d)\n",
       "       10    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:57(_path_join)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
       "        3    0.000    0.000    0.000    0.000 header.py:694(clear)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:237(mgr_locs)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:1688(is_extension_array_dtype)\n",
       "       14    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "       23    0.000    0.000    0.000    0.000 _internal.py:309(_getfield_is_safe)\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "       41    0.000    0.000    0.000    0.000 {built-in method _weakref._remove_dead_weakref}\n",
       "       40    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 keplerml.py:299(<listcomp>)\n",
       "       11    0.000    0.000    0.000    0.000 hdulist.py:477(index_of)\n",
       "        7    0.000    0.000    0.000    0.000 file.py:321(tell)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3488(_verify_integrity)\n",
       "       15    0.000    0.000    0.000    0.000 common.py:1835(_get_dtype_type)\n",
       "        9    0.000    0.000    0.000    0.000 numeric.py:2967(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'newbyteorder' of 'numpy.dtype' objects}\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "       60    0.000    0.000    0.000    0.000 internals.py:4990(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1207(construct_1d_object_array_from_listlike)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _csv.writer}\n",
       "        2    0.000    0.000    0.000    0.000 linalg.py:139(_commonType)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'clip' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 table.py:788(match_header)\n",
       "        1    0.000    0.000    0.000    0.000 hdulist.py:682(close)\n",
       "        4    0.000    0.000    0.000    0.000 header.py:1977(_block_size)\n",
       "        1    0.000    0.000    0.000    0.000 six.py:482(iteritems)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:650(isiterable)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1590(is_object)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:61(is_dtype)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.generic' objects}\n",
       "        2    0.000    0.000    0.000    0.000 type_check.py:18(mintypecode)\n",
       "       19    0.000    0.000    0.000    0.000 enum.py:517(__new__)\n",
       "        1    0.000    0.000    0.000    0.000 _weakrefset.py:58(__iter__)\n",
       "       21    0.000    0.000    0.000    0.000 table.py:155(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 column.py:1162(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 missing.py:112(_isna_new)\n",
       "       19    0.000    0.000    0.000    0.000 _methods.py:31(_sum)\n",
       "        2    0.000    0.000    0.000    0.000 _internal.py:341(_view_is_safe)\n",
       "        1    0.000    0.000    0.000    0.000 posixpath.py:338(normpath)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
       "       40    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:270(_save)\n",
       "        1    0.000    0.000    0.034    0.034 table.py:397(data)\n",
       "        2    0.000    0.000    0.000    0.000 image.py:106(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 fitsrec.py:1022(_get_scale_factors)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:967(_is_append_mode_platform)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3191(make_block)\n",
       "        1    0.000    0.000    0.002    0.002 frame.py:7349(_arrays_to_mgr)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:510(_shallow_copy)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.arange}\n",
       "        6    0.000    0.000    0.000    0.000 numeric.py:2064(isscalar)\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:394(urlsplit)\n",
       "       10    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:59(<listcomp>)\n",
       "      6/2    0.000    0.000    0.001    0.000 {built-in method builtins.__import__}\n",
       "       16    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
       "        1    0.000    0.000    0.026    0.026 hdulist.py:254(fromfile)\n",
       "        1    0.000    0.000    0.000    0.000 nonstandard.py:107(match_header)\n",
       "        1    0.000    0.000    0.000    0.000 column.py:1453(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:260(__delete__)\n",
       "        4    0.000    0.000    0.000    0.000 util.py:971(fileobj_is_binary)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:3307(shape)\n",
       "        7    0.000    0.000    0.000    0.000 common.py:477(is_interval_dtype)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:513(is_categorical_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 linalg.py:180(_fastCopyAndTranspose)\n",
       "        2    0.000    0.000    0.000    0.000 getlimits.py:94(__new__)\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:23(find_module)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method fcntl.fcntl}\n",
       "        1    0.000    0.000    0.000    0.000 genericpath.py:117(_splitext)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "       10    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "       19    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}\n",
       "        3    0.000    0.000    0.000    0.000 table.py:59(match_header)\n",
       "        4    0.000    0.000    0.000    0.000 hdulist.py:168(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 fitsrec.py:624(__del__)\n",
       "        1    0.000    0.000    0.000    0.000 column.py:1451(_recformats)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1744(match_header)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:801(fileobj_mode)\n",
       "        6    0.000    0.000    0.000    0.000 _util.py:51(<genexpr>)\n",
       "        1    0.000    0.000    0.002    0.002 frame.py:7604(_convert_object_array)\n",
       "        2    0.000    0.000    0.000    0.000 linalg.py:168(_to_native_byte_order)\n",
       "        2    0.000    0.000    0.000    0.000 function_base.py:1963(place)\n",
       "        3    0.000    0.000    0.000    0.000 shape_base.py:180(vstack)\n",
       "       38    0.000    0.000    0.000    0.000 inspect.py:2502(name)\n",
       "        1    0.000    0.000    0.000    0.000 posixpath.py:121(splitext)\n",
       "        1    0.000    0.000    0.000    0.000 posixpath.py:376(abspath)\n",
       "       28    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "        1    0.000    0.000    0.000    0.000 image.py:1010(match_header)\n",
       "        3    0.000    0.000    0.000    0.000 fitsrec.py:1270(_get_recarray_field)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:851(_fileobj_is_append_mode)\n",
       "        3    0.000    0.000    0.000    0.000 __init__.py:205(iteritems)\n",
       "       17    0.000    0.000    0.000    0.000 _methods.py:37(_any)\n",
       "        3    0.000    0.000    0.000    0.000 shape_base.py:230(<listcomp>)\n",
       "       40    0.000    0.000    0.000    0.000 {built-in method numpy.core.umath.geterrobj}\n",
       "        1    0.000    0.000    0.000    0.000 {method '__setstate__' of 'numpy.dtype' objects}\n",
       "        6    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)\n",
       "        1    0.000    0.000    0.000    0.000 _weakrefset.py:36(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1149(find_spec)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:185(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 six.py:184(find_module)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
       "        7    0.000    0.000    0.000    0.000 {method 'start' of '_sre.SRE_Match' objects}\n",
       "       28    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "        1    0.000    0.000    0.678    0.678 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 table.py:468(_theap)\n",
       "        1    0.000    0.000    0.000    0.000 convenience.py:772(_get_file_mode)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:502(_get_raw_data)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:785(fileobj_closed)\n",
       "        2    0.000    0.000    0.000    0.000 py3compat.py:86(fileobj_open)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:520(UnicodeWriter)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:2052(to_native_types)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2408(to_native_types)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:332(is_datetime64_dtype)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:546(is_string_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:1043(is_datetime64_any_dtype)\n",
       "        3    0.000    0.000    0.000    0.000 twodim_base.py:244(diag)\n",
       "        8    0.000    0.000    0.000    0.000 linalg.py:111(isComplexType)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:1206(diagonal)\n",
       "        1    0.000    0.000    0.000    0.000 memmap.py:283(__array_finalize__)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'round' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 type_check.py:237(iscomplexobj)\n",
       "        2    0.000    0.000    0.000    0.000 numeric.py:148(ones)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:12(__new__)\n",
       "        2    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "       35    0.000    0.000    0.000    0.000 inspect.py:2506(default)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        3    0.000    0.000    0.000    0.000 column.py:1825(_get_index)\n",
       "        3    0.000    0.000    0.000    0.000 util.py:1229(_pad_length)\n",
       "        2    0.000    0.000    0.000    0.000 py3compat.py:108(isfile)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:444(is_period_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.where}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray._insert}\n",
       "        1    0.000    0.000    0.000    0.000 genericpath.py:16(exists)\n",
       "        1    0.000    0.000    0.000    0.000 _bootlocale.py:23(getpreferredencoding)\n",
       "       10    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "       18    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'index' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 column.py:1298(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 file.py:563(_is_random_access_file_backed)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:821(_fileobj_normalize_mode)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3784(_consolidate_check)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:356(ftype)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2445(equals)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:672(values)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:920(_get_attributes_dict)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:251(is_list_like)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 type_check.py:61(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 posixpath.py:64(isabs)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
       "       20    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        1    0.000    0.000    0.001    0.001 image.py:963(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 table.py:461(_nrows)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3785(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4101(_consolidate_inplace)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:677(_values)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:858(is_signed_integer_dtype)\n",
       "        5    0.000    0.000    0.000    0.000 common.py:1784(_get_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 linalg.py:124(_realType)\n",
       "        2    0.000    0.000    0.000    0.000 linalg.py:192(_assertRank2)\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:1490(nonzero)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'diagonal' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.result_type}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method math.log}\n",
       "        1    0.000    0.000    0.000    0.000 _weakrefset.py:26(__exit__)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1080(_path_importer_cache)\n",
       "       12    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:847(__exit__)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:75(_path_stat)\n",
       "      6/2    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:211(_call_with_frames_removed)\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}\n",
       "       16    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 image.py:140(match_header)\n",
       "        3    0.000    0.000    0.000    0.000 image.py:179(header)\n",
       "        1    0.000    0.000    0.000    0.000 image.py:189(data)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:635(_reset_identity)\n",
       "        9    0.000    0.000    0.000    0.000 base.py:641(__len__)\n",
       "        3    0.000    0.000    0.000    0.000 missing.py:32(isna)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:407(is_timedelta64_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:1490(is_string_like_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:1578(is_bool_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 inference.py:119(is_iterator)\n",
       "        6    0.000    0.000    0.000    0.000 stride_tricks.py:193(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 linalg.py:106(_makearray)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:1686(clip)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'conj' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 csvs.py:106(<genexpr>)\n",
       "       19    0.000    0.000    0.000    0.000 inspect.py:2514(kind)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "       12    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:843(__enter__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'startswith' of 'bytes' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'issubset' of 'set' objects}\n",
       "       11    0.000    0.000    0.000    0.000 {function HDUList.__getitem__ at 0x7f72c1935e18}\n",
       "        3    0.000    0.000    0.000    0.000 fitsrec.py:580(columns)\n",
       "        3    0.000    0.000    0.000    0.000 column.py:771(ascii)\n",
       "        1    0.000    0.000    0.000    0.000 column.py:1459(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:230(_data_loaded)\n",
       "        2    0.000    0.000    0.000    0.000 internals.py:3490(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 internals.py:233(mgr_locs)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:89(is_object_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 stride_tricks.py:189(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 linalg.py:130(_linalgRealType)\n",
       "        2    0.000    0.000    0.000    0.000 numeric.py:676(<genexpr>)\n",
       "       19    0.000    0.000    0.000    0.000 inspect.py:2510(annotation)\n",
       "        3    0.000    0.000    0.000    0.000 inspect.py:2817(parameters)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:780(find_spec)\n",
       "       16    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'partition' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:190(_save_header)\n",
       "        1    0.000    0.000    0.000    0.000 table.py:377(match_header)\n",
       "        1    0.000    0.000    0.000    0.000 column.py:1455(_dims)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:234(_has_data)\n",
       "        2    0.000    0.000    0.000    0.000 _util.py:52(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:95(_expand_user)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3266(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:127(_check_ndim)\n",
       "        2    0.000    0.000    0.000    0.000 internals.py:348(shape)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:647(__array__)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1325(nlevels)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:907(is_unsigned_integer_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:1527(is_float_dtype)\n",
       "        3    0.000    0.000    0.000    0.000 dtypes.py:584(is_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.promote_types}\n",
       "        2    0.000    0.000    0.000    0.000 type_check.py:63(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)\n",
       "        1    0.000    0.000    0.000    0.000 _weakrefset.py:16(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 _weakrefset.py:20(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 _weakrefset.py:52(_commit_removals)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'fileno' of '_io.FileIO' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
       "        2    0.000    0.000    0.000    0.000 image.py:491(_verify_blank)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:751(_close)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:757(fileobj_name)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:5026(_shape_compat)\n",
       "        9    0.000    0.000    0.000    0.000 internals.py:3309(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3311(ndim)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3776(is_consolidated)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:615(is_)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:662(dtype)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:369(is_datetime64tz_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:1170(is_datetime_or_timedelta_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:1405(needs_i8_conversion)\n",
       "        4    0.000    0.000    0.000    0.000 {method '__array_prepare__' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'item' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:213(setstate)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:707(find_spec)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:41(_relax_case)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3384(_get_items)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:352(dtype)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:922(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:388(is_named_tuple)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1574(shape)\n",
       "        3    0.000    0.000    0.000    0.000 inspect.py:2821(return_annotation)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun fc.features_from_fits(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfile = testfile\n",
    "t, nf, err = fc.read_kepler_curve(nfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(t)==len(nf) and len(t)==len(err), \"t, nf, err arrays must be equal sizes.\"\n",
    "\n",
    "\"\"\"\n",
    "The essentials which probably can't be improved much\n",
    "\"\"\"\n",
    "\n",
    "longtermtrend = np.polyfit(t, nf, 1)[0] # Feature 1 (Abbr. F1) overall slope\n",
    "yoff = np.polyfit(t, nf, 1)[1] # Not a feature, y-intercept of linear fit\n",
    "nf_mean = np.mean(nf)\n",
    "nf_med = np.median(nf)\n",
    "stds = np.std(nf) #f6\n",
    "meanmedrat = nf_mean / nf_med # F2\n",
    "skews = stats.skew(nf) # F3\n",
    "varss = np.var(nf) # F4\n",
    "coeffvar = stds/nf_mean #F5\n",
    "\n",
    "corrnf = nf - longtermtrend*t - yoff #this removes any linear trend of lc so you can look at just troughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Outlying flux counts\n",
    "\"\"\"\n",
    "\n",
    "posthreshold = nf_mean+4*stds\n",
    "negthreshold = nf_mean-4*stds\n",
    "\n",
    "numout1s = len(nf[np.abs(nf-nf_mean)>stds])\n",
    "numposoutliers = len(nf[nf>posthreshold])\n",
    "numnegoutliers = len(nf[nf<negthreshold])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "numoutliers=numposoutliers+numnegoutliers #F10\n",
    "\n",
    "kurt = stats.kurtosis(nf)\n",
    "\n",
    "mad = np.median(np.abs(nf-nf_med))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1 µs ± 1.14 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "4.61 ms ± 26.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# delta nf/delta t\n",
    "%timeit slopes1 = (nf[1:]-nf[:-1])/(t[1:]-t[:-1])\n",
    "\n",
    "%timeit slopes2 =[(nf[j+1]-nf[j])/(t[j+1]-t[j]) for j in range(len(nf)-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = (nf[1:]-nf[:-1])/(t[1:]-t[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 µs ± 45.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "923 µs ± 31.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (corrnf[1:]-corrnf[:-1])/(t[1:]-t[:-1])\n",
    "%timeit [(corrnf[j+1]-corrnf[j])/(t[j+1]-t[j]) for j in range (len(corrnf)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrslopes (corrected slopes) removes the longterm linear trend (if any) and then looks at the slope\n",
    "corrslopes = (corrnf[1:]-corrnf[:-1])/(t[1:]-t[:-1])\n",
    "meanslope = np.mean(slopes) #F12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by looking at where the 99th percentile is instead of just the largest number,\n",
    "# I think it avoids the extremes which might not be relevant (might be unreliable data)\n",
    "# Is the miniumum slope the most negative one, or the flattest one? Answer: Most negative\n",
    "maxslope=np.percentile(slopes,99) #F13\n",
    "minslope=np.percentile(slopes,1) #F14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating positive slopes and negative slopes\n",
    "# Should both include the 0 slope? It doesn't matter for calculating the means later on...\n",
    "pslope = slopes[slopes>=0]\n",
    "nslope = slopes[slopes<=0]\n",
    "# Looking at the average (mean) positive and negative slopes\n",
    "\n",
    "if len(pslope)==0:\n",
    "    meanpslope=0\n",
    "else:\n",
    "    meanpslope=np.mean(pslope) #F15\n",
    "\n",
    "if len(nslope)==0:\n",
    "    meannslope=0\n",
    "else:\n",
    "    meannslope=np.mean(nslope) #F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifying the difference in shape.\n",
    "if meannslope==0:\n",
    "    # if meannslope==0 (i.e., if there are no negative slopes), g_asymm is assigned a value of 10\n",
    "    # This value is chosen such that \n",
    "    # a) it is positive (where g_asymm is inherently negative), \n",
    "    # b) it is a factor larger than a random signal would produce (roughly equal average of positive and negative slopes -> g_asymm=-1)\n",
    "    # c) it is not orders of magnitude larger than other data, which would affect outlier analysis\n",
    "    g_asymm = 10 \n",
    "else:\n",
    "    g_asymm=meanpslope / meannslope #F17\n",
    "\n",
    "# Won't this be skewed by the fact that both pslope and nslope have all the 0's? Eh\n",
    "if len(nslope)==0:\n",
    "    rough_g_asymm=10\n",
    "else:\n",
    "    rough_g_asymm=len(pslope) / len(nslope) #F18\n",
    "\n",
    "# meannslope is inherently negative, so this is the difference btw the 2\n",
    "diff_asymm=meanpslope + meannslope #F19\n",
    "skewslope = stats.skew(slopes) #F20\n",
    "absslopes = np.abs(slopes)\n",
    "meanabsslope = np.mean(absslopes) #F21\n",
    "varabsslope = np.var(absslopes) #F22\n",
    "varslope = np.var(slopes) #F23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secder = Second Derivative\n",
    "# Reminder for self: the slope is \"located\" halfway between the flux and time points, \n",
    "# the delta t in the denominator accounts for that.\n",
    "# secder = delta slopes/delta t, delta t = ((t_j-t_(j-1))+(t_(j+1)-t_j))/2\n",
    "# secder=[(slopes[j]-slopes[j-1])/((t[j+1]-t[j])/2+(t[j]-t[j-1])/2) for j in range(1, len(slopes)-1)]\n",
    "# after algebraic simplification:\n",
    "secder = 2*(slopes[1:]-slopes[:-1])/(t[1:-1]-t[:-2])\n",
    "\n",
    "# abssecder=[abs((slopes[j]-slopes[j-1])/((t[j+1]-t[j])/2+(t[j]-t[j-1])/2)) for j in range (1, len(slopes)-1)]\n",
    "# simplification:\n",
    "\n",
    "abssecder=np.abs(np.array(secder))\n",
    "absmeansecder=np.mean(abssecder) #F24\n",
    "if len(pslope)==0:\n",
    "    pslopestds=0\n",
    "else:\n",
    "    pslopestds=np.std(pslope)\n",
    "if len(nslope)==0:\n",
    "    nslopesstds=0\n",
    "    stdratio=10 # the ratio for noise-only variation will be ~1, so a single order of magnitude seems sufficient.\n",
    "else:\n",
    "    nslopestds=np.std(nslope)\n",
    "    stdratio=pslopestds/nslopestds\n",
    "\n",
    "sdstds=np.std(secder)\n",
    "meanstds=np.mean(secder)\n",
    "\n",
    "num_psdspikes,num_nsdspikes=0,0\n",
    "\n",
    "num_pspikes=len(slopes[slopes>=meanpslope+3*pslopestds]) #F25\n",
    "num_nspikes=len(slopes[slopes<=meannslope-3*nslopestds]) #F26\n",
    "\n",
    "num_psdspikes = len(secder[secder>=4*sdstds]) #F27\n",
    "num_nsdspikes = len(secder[secder<=4*sdstds]) #F28\n",
    "\n",
    "if nslopestds==0:\n",
    "    stdratio=10\n",
    "else:\n",
    "    stdratio = pslopestds / nslopestds #F29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ratio of postive slopes with a following postive slope to the total number of points.\n",
    "pairs = np.where((slopes[1:]>0)&(slopes[:-1]>0))[0] # where positive slopes are followed by another positive slope\n",
    "pstrend=len(pairs)/len(slopes) #F30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the flux crosses the 'zero' line.\n",
    "zcrossind_alt = np.where(corrnf[:-1]*corrnf[1:]<0)\n",
    "num_zcross = len(zcrossind) #F31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "plusminus = np.where((slopes[1:]<0)&(slopes[:-1]>0))[0]\n",
    "num_pm = len(plusminus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks up the local maximums. Adds a peak if it's the largest within 10 points on either side.\n",
    "# Q: Is there a way to do this and take into account drastically different periodicity scales?\n",
    "naivemax,nmax_times = [],[]\n",
    "naivemins,nmin_times = [],[]\n",
    "\n",
    "for j in range(len(nf)):\n",
    "    if nf[j] == max(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "        naivemax.append(nf[j])\n",
    "        nmax_times.append(t[j])\n",
    "    elif nf[j] == min(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "        naivemins.append(nf[j])\n",
    "        nmin_times.append(t[j])\n",
    "        \n",
    "naivemax = np.array(naivemax)\n",
    "nmax_times = np.array(nmax_times)\n",
    "naivemins = np.array(naivemins)\n",
    "nmin_times = np.array(nmin_times)\n",
    "len_nmax=len(naivemax) #F33\n",
    "len_nmin=len(naivemins) #F34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(naivemax)>2:\n",
    "    mautocorrcoef = np.corrcoef(naivemax[:-1], naivemax[1:])[0][1] #F35\n",
    "else:\n",
    "    mautocorrcoef = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"peak to peak slopes\"\"\"\n",
    "#ppslopes = [abs((naivemax[j+1]-naivemax[j])/(nmax_times[j+1]-nmax_times[j])) \\\n",
    "#            for j in range(len(naivemax)-1)]\n",
    "ppslopes = (naivemax[1:]-naivemax[:-1])/(nmax_times[1:]-nmax_times[:-1])\n",
    "if len(ppslopes)==0:\n",
    "    ptpslopes = 0\n",
    "else:\n",
    "    ptpslopes=np.mean(ppslopes) #F36\n",
    "\n",
    "#maxdiff=[nmax_times[j+1]-nmax_times[j] for j in range(len(naivemax)-1)]\n",
    "maxdiff = nmax_times[1:]-nmax_times[:-1]\n",
    "if len(maxdiff)==0:\n",
    "    periodicity=0\n",
    "    periodicityr=0\n",
    "    naiveperiod=0\n",
    "else:\n",
    "    periodicity=np.std(maxdiff)/np.mean(maxdiff) #F37\n",
    "    periodicityr=np.sum(abs(maxdiff-np.mean(maxdiff)))/np.mean(maxdiff) #F38\n",
    "    naiveperiod=np.mean(maxdiff) #F39\n",
    "if len(naivemax)==0:\n",
    "    maxvars=0\n",
    "    maxvarsr=0\n",
    "else:\n",
    "    maxvars = np.std(naivemax)/np.mean(naivemax) #F40\n",
    "    maxvarsr = np.sum(abs(naivemax-np.mean(naivemax)))/np.mean(naivemax) #F41\n",
    "\n",
    "emin = naivemins[::2] # even indice minimums\n",
    "omin = naivemins[1::2] # odd indice minimums\n",
    "meanemin = np.mean(emin)\n",
    "if len(omin)==0:\n",
    "    meanomin=0\n",
    "else:\n",
    "    meanomin = np.mean(omin)\n",
    "oeratio = meanomin/meanemin #F42\n",
    "\n",
    "# amp here is actually amp_2 in revantese\n",
    "# 2x the amplitude (peak-to-peak really)\n",
    "amp = np.percentile(nf,99)-np.percentile(nf,1) #F43\n",
    "normamp = amp / np.mean(nf) #this should prob go, since flux is norm'd #F44\n",
    "\n",
    "# ratio of points within one fifth of the amplitude to the median to total number of points \n",
    "#mbp = len(nf[(nf<=(np.median(nf)+0.1*amp))&(nf>=(np.median(nf)-0.1*amp))]) / len(nf) #F45\n",
    "mbp = len(nf[(nf<=(nf_med+0.1*amp))&(nf>=(nf_med-0.1*amp))]) / len(nf) #F45\n",
    "f595 = np.percentile(nf,95)-np.percentile(nf,5)\n",
    "f1090 =np.percentile(nf,90)-np.percentile(nf,10)\n",
    "f1782 =np.percentile(nf, 82)-np.percentile(nf, 17)\n",
    "f2575 =np.percentile(nf, 75)-np.percentile(nf, 25)\n",
    "f3267 =np.percentile(nf, 67)-np.percentile(nf, 32)\n",
    "f4060 =np.percentile(nf, 60)-np.percentile(nf, 40)\n",
    "mid20 =f4060/f595 #F46\n",
    "mid35 =f3267/f595 #F47\n",
    "mid50 =f2575/f595 #F48\n",
    "mid65 =f1782/f595 #F49\n",
    "mid80 =f1090/f595 #F50 \n",
    "\n",
    "percentamp = max(np.abs(nf-nf_med)/nf_med) #F51\n",
    "magratio = (max(nf)-nf_med) / amp #F52\n",
    "\n",
    "autocorrcoef = np.corrcoef(nf[:-1], nf[1:])[0][1] #F53\n",
    "\n",
    "sautocorrcoef = np.corrcoef(slopes[:-1], slopes[1:])[0][1] #F54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5 µs ± 44.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "211 ms ± 24.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit arr1 = nf[(nf<=(nf_med+0.1*amp))&(nf>=(nf_med-0.1*amp))]\n",
    "%timeit arr2 = [nf[j] for j in range(len(nf)) if (nf[j] < (np.median(nf) + 0.1*amp)) & (nf[j] > (np.median(nf)-0.1*amp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(arr1,arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.2 µs ± 5.02 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "neighbors = 5\n",
    "flat = np.zeros_like(slopes)\n",
    "flat[neighbors:-neighbors+1] = slopes[:-2*neighbors+1]\n",
    "flat[neighbors-1:-neighbors] -= slopes[2*neighbors-1:]\n",
    "for i in range(1,neighbors):\n",
    "    flat[neighbors:-neighbors+1]+=slopes[i:-2*neighbors+i+1]\n",
    "    flat[neighbors-1:-neighbors]-=slopes[2*neighbors-i-1:-i]\n",
    "flat = flat/(neighbors)\n",
    "flat = flat[nmax_inds[(nmax_inds>5)&(nmax_inds<len(slopes)-5)]-1] # all the inds are off by one, I don't know why and I don't care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.84 ms ± 3.87 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "flat1 = [np.mean(slopes[max(0,j-6):min(j-1, len(slopes)-1):1]\\\n",
    "        - slopes[max(0,j):min(j+5, len(slopes)-1):1])\\\n",
    "        for j in nmax_inds if (j>5)&(j<len(slopes)-5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Deprecated. Replaced by method call in following cells.\n",
    "# this convoluted crap is about 35x faster than what was here before.\n",
    "\n",
    "nmax_inds = np.argsort(t)[np.searchsorted(t,nmax_times,sorter=np.argsort(t))]\n",
    "flat = np.zeros_like(slopes)\n",
    "flat[neighbors:-neighbors+1] = slopes[:-2*neighbors+1]\n",
    "flat[neighbors-1:-neighbors] -= slopes[2*neighbors-1:]\n",
    "for i in range(1,neighbors):\n",
    "    flat[neighbors:-neighbors+1]+=slopes[i:-2*neighbors+i+1]\n",
    "    flat[neighbors-1:-neighbors]-=slopes[2*neighbors-i-1:-i]\n",
    "flat = flat/(neighbors)\n",
    "flatness = flat[nmax_inds[(nmax_inds>5)&(nmax_inds<len(slopes)-5)]-1] \n",
    "# all the inds are off by one, I don't know why. I corrected it with a -1 at the end.\n",
    "# The mixture of indices is messing with my head and I don't know exactly what needs changed.\n",
    "# At least all of the mismatches are aligned and results are consistent with previous work.\n",
    "# Fixed in method call below\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 µs ± 50.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Deprecated\n",
    "%%timeit\n",
    "flat = mean_n_neighbors(slopes,nbrs=5,comb_method='+-')\n",
    "flatness = flat[nmax_inds[(nmax_inds>5)&(nmax_inds<len(slopes)-5)]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference, the old code (itself as optimized as I can make it):\n",
    "\"\"\"\n",
    "flatness = [np.mean(slopes[max(0,j-6):min(j-1, len(slopes)-1):1]\\\n",
    "        - slopes[max(0,j):min(j+5, len(slopes)-1):1])\\\n",
    "        for j in nmax_inds if (j>5)&(j<len(slopes)-5)]\n",
    "\"\"\"\n",
    "# njit beats this handily.\n",
    "def mean_n_neighbors(nparr,nbrs=5,comb_method='++'):\n",
    "    average_arr = np.zeros_like(nparr)\n",
    "    exec(\"average_arr[neighbors+1:-neighbors+1] {}= nparr[:-2*neighbors]\".format(comb_method[0]))\n",
    "    exec(\"average_arr[neighbors+1:-neighbors+1] {}= nparr[2*neighbors:]\".format(comb_method[1]))\n",
    "    for i in range(1,neighbors):\n",
    "        exec(\"average_arr[neighbors+1:-neighbors+1]{}=nparr[i:-2*neighbors+i]\".format(comb_method[0]))\n",
    "        exec(\"average_arr[neighbors+1:-neighbors+1]{}=nparr[2*neighbors-i:-i]\".format(comb_method[1]))\n",
    "    average_arr = average_arr/neighbors # right now is just over neighbors to match original definitition, \n",
    "                                        # should divde by another factor of 2 for the actual mean...\n",
    "    return average_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def flat_func(slopes,inds):\n",
    "    flat1 = np.zeros(len(inds))\n",
    "    for i,j in enumerate(inds):\n",
    "        flat1[i] = np.mean(slopes[j-6:j-1])-np.mean(slopes[j:j+5])\n",
    "    return flat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.4 µs ± 95.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "nmax_inds_subset = nmax_inds[(nmax_inds>5)&(nmax_inds<len(slopes)-5)]\n",
    "flatness = flat_func(slopes,nmax_inds_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(flatness)==0: flatmean=0\n",
    "else: flatmean = np.mean(flatness) #F55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Optimized and Deprecated\n",
    "nmin_inds\n",
    "tflatness = [np.mean(-slopes[j-6:j-1]+ slopes[j:j+5])\\\n",
    "             for j in nmin_inds if (j>5)&(j<len(slopes-5))] \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "tflat = mean_n_neighbors(slopes,nbrs=5,comb_method='-+')\n",
    "tflatness = tflat[nmin_inds[(nmin_inds>5)&(nmin_inds<len(slopes)-5)]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def tflat_func(slopes,inds):\n",
    "    tflat1 = np.zeros(len(inds))\n",
    "    for i,j in enumerate(inds):\n",
    "        tflat1[i] = -np.mean(slopes[j-6:j-1])+np.mean(slopes[j:j+5])\n",
    "    return tflat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.4 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "nmin_inds_subset = nmin_inds[(nmin_inds>5)&(nmin_inds<len(slopes)-5)]\n",
    "tflatness = tflat_func(slopes,nmin_inds_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tflatness for mins, flatness for maxes\n",
    "if len(tflatness)==0: tflatmean=0\n",
    "else: tflatmean = np.mean(tflatness) #F56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bookmark\n",
    "\n",
    "I'm inclined to believe that roundness and troundness can be optimized in the same way that flatness and tflatness were. If so, that'd be greeaaat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, njit, vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.79 ms ± 654 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Original\n",
    "roundness = [np.mean(secder[max(0,j-6):j:1])\\\n",
    "              +np.mean(secder[j:min(j+6,len(secder)-1):1])\\\n",
    "              for j in range(6,len(secder)-6)\\\n",
    "              if t[j] in nmax_times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Original made better\n",
    "roundness1 = [np.mean(secder[j-6:j+6])*2\\\n",
    "              for j in nmax_inds if (j>5)&(j<len(secder)-5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np.around(roundness,decimals=10),np.around(roundness1,decimals=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def roundness_func(secder,inds):\n",
    "    roundness1 = np.zeros(len(inds))\n",
    "    for i,j in enumerate(inds):\n",
    "        roundness1[i] = np.mean(secder[j-6:j+6])*2\n",
    "    return roundness1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.9 µs ± 1.67 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "nmax_inds_subset = nmax_inds[(nmax_inds>5)&(nmax_inds<len(secder-5))]\n",
    "roundness2 = roundness_func(secder,nmax_inds_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np.around(roundness,decimals=10),np.around(roundness2,decimals=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax_inds_subset = nmax_inds[(nmax_inds>5)&(nmax_inds<len(secder-5))]\n",
    "roundness = roundness_func(secder,nmax_inds_subset)\n",
    "if len(roundness)==0: roundmean=0\n",
    "else: roundmean = np.mean(roundness) #F57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.76 ms ± 500 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# old\n",
    "troundness = [np.mean(secder[max(0,j-6):j])\\\n",
    "              +np.mean(secder[j:min(j+6,len(secder)-1)])\\\n",
    "              for j in range(6,len(secder)-6)\\\n",
    "              if t[j] in nmin_times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.1 µs ± 1.87 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "nmin_inds_subset = nmin_inds[(nmin_inds>5)&(nmin_inds<len(secder-5))]\n",
    "troundness1 = roundness_func(secder,nmin_inds_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np.around(troundness,decimals=10),np.around(troundness1,decimals=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmin_inds_subset = nmin_inds[(nmin_inds>5)&(nmin_inds<len(secder-5))]\n",
    "troundness = roundness_func(secder,nmin_inds_subset)\n",
    "if len(troundness)==0:troundmean=0\n",
    "else:troundmean = np.mean(troundness) #F58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if troundmean==0 and roundmean==0:roundrat=1\n",
    "elif troundmean==0:roundrat=10\n",
    "else:roundrat = roundmean / troundmean #F59\n",
    "\n",
    "if flatmean==0 and tflatmean==0:flatrat=1\n",
    "elif tflatmean==0:flatrat=10\n",
    "else:flatrat = flatmean / tflatmean #F60\n",
    "\n",
    "ndata = np.array([longtermtrend, meanmedrat, skews, varss, coeffvar, stds, \\\n",
    "         numoutliers, numnegoutliers, numposoutliers, numout1s, kurt, mad, \\\n",
    "         maxslope, minslope, meanpslope, meannslope, g_asymm, rough_g_asymm, \\\n",
    "         diff_asymm, skewslope, varabsslope, varslope, meanabsslope, absmeansecder, \\\n",
    "         num_pspikes, num_nspikes, num_psdspikes, num_nsdspikes,stdratio, pstrend, \\\n",
    "         num_zcross, num_pm, len_nmax, len_nmin, mautocorrcoef, ptpslopes, \\\n",
    "         periodicity, periodicityr, naiveperiod, maxvars, maxvarsr, oeratio, \\\n",
    "         amp, normamp, mbp, mid20, mid35, mid50, \\\n",
    "         mid65, mid80, percentamp, magratio, sautocorrcoef, autocorrcoef, \\\n",
    "         flatmean, tflatmean, roundmean, troundmean, roundrat, flatrat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts = [\"longtermtrend\", \"meanmedrat\", \"skews\", \"varss\", \"coeffvar\", \"stds\", \\\n",
    "       \"numoutliers\", \"numnegoutliers\", \"numposoutliers\", \"numout1s\", \"kurt\", \"mad\", \\\n",
    "       \"maxslope\", \"minslope\", \"meanpslope\", \"meannslope\", \"g_asymm\", \"rough_g_asymm\", \\\n",
    "       \"diff_asymm\", \"skewslope\", \"varabsslope\", \"varslope\", \"meanabsslope\", \"absmeansecder\", \\\n",
    "       \"num_pspikes\", \"num_nspikes\", \"num_psdspikes\", \"num_nsdspikes\",\"stdratio\", \"pstrend\", \\\n",
    "       \"num_zcross\", \"num_pm\", \"len_nmax\", \"len_nmin\", \"mautocorrcoef\", \"ptpslopes\", \\\n",
    "       \"periodicity\", \"periodicityr\", \"naiveperiod\", \"maxvars\", \"maxvarsr\", \"oeratio\", \\\n",
    "       \"amp\", \"normamp\", \"mbp\", \"mid20\", \"mid35\", \"mid50\", \\\n",
    "       \"mid65\", \"mid80\", \"percentamp\", \"magratio\", \"sautocorrcoef\", \"autocorrcoef\", \\\n",
    "       \"flatmean\", \"tflatmean\", \"roundmean\", \"troundmean\", \"roundrat\", \"flatrat\"]\n",
    "\n",
    "df = pd.DataFrame(index=[nfile[nfile.find('kplr'):nfile.find('_llc')]],data=[ndata],columns=fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_fts = ['ft1','ft2','ft3']\n",
    "fake_data = [range(3),range(3),range(3),range(3)]\n",
    "fake_df = pd.DataFrame(data=fake_data,columns=fake_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft1</th>\n",
       "      <th>ft2</th>\n",
       "      <th>ft3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ft1  ft2  ft3\n",
       "0    0    1    2\n",
       "1    0    1    2\n",
       "2    0    1    2\n",
       "3    0    1    2"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df0 = fake_df.iloc[0]\n",
    "fake_df1 = fake_df.iloc[1]\n",
    "fake_df2 = fake_df.iloc[2]\n",
    "fake_df3 = fake_df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fake_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tmp.p','ab') as f:\n",
    "    pickle.dump(fake_df0,f)\n",
    "    pickle.dump(fake_df1,f)\n",
    "    pickle.dump(fake_df2,f)\n",
    "    pickle.dump(fake_df3,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('./tmp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = pickle.load(open('./tmp.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df_r = pd.DataFrame()\n",
    "with open('./tmp.p','rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            fake_df_r = fake_df_r.append(pickle.load(fr))\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft1</th>\n",
       "      <th>ft2</th>\n",
       "      <th>ft3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ft1  ft2  ft3\n",
       "0  0.0  1.0  2.0\n",
       "1  0.0  1.0  2.0\n",
       "2  0.0  1.0  2.0\n",
       "3  0.0  1.0  2.0"
      ]
     },
     "execution_count": 980,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 982,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fake_df_r.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df_r = fake_df_r.append(pickle.load(open('./tmp.p','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft1</th>\n",
       "      <th>ft2</th>\n",
       "      <th>ft3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ft1  ft2  ft3\n",
       "0  0.0  1.0  2.0\n",
       "0  0.0  1.0  2.0\n",
       "0  0.0  1.0  2.0"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(open('tmp_data.csv','w'))\n",
    "pickle.dump(df,open('tmp_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = pd.read_csv(open('tmp_data.csv','r'),index_col=0)\n",
    "df_rp = pickle.load(open('tmp_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 992,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.equals(df,df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 993,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.equals(df,df_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpfile='Q8_tmp_data.p'\n",
    "df = pd.DataFrame()\n",
    "with open(tmpfile, 'rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            df = df.append(pickle.load(fr))\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpfile='Q8_tmp_data.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "with open(tmpfile, 'rb') as fr:\n",
    "    try:\n",
    "        i=0\n",
    "        while i<10000:\n",
    "            df = df.append(pickle.load(fr))\n",
    "            i+=1\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6303024"
      ]
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(i):\n",
    "    \"\"\"\n",
    "    Reads in the finished data file (tmp_data.p by default), sorts it, and saves it to the\n",
    "    specified output csv. Effectively just sorting a csv and renaming it.\n",
    "    \"\"\"\n",
    "    out_file = 'Q{}_output.p'.format(i)\n",
    "    tmpfile = 'Q{}_tmp_data.p'.format(i)\n",
    "    df = pd.DataFrame()\n",
    "    with open(tmpfile, 'rb') as fr:\n",
    "        try:\n",
    "            j=0\n",
    "            while True:\n",
    "                j+=1\n",
    "                if j%1000==0:\n",
    "                    print(\"Q{}: {}/??? loaded\".format(i,j))\n",
    "                df = df.append(pickle.load(fr))\n",
    "        except EOFError:\n",
    "            pass\n",
    "    df = df.sort_index()\n",
    "    pickle.dump(df,open(out_file,'wb'))\n",
    "    os.remove(tmpfile)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def n_mins_maxes(t,nf):\n",
    "    naivemax,nmax_times,nmax_inds = [],[],[]\n",
    "    naivemins,nmin_times,nmin_inds = [],[],[]\n",
    "    for j in range(len(nf)):\n",
    "        nfj = nf[j]\n",
    "        if j-10<0:\n",
    "            jmin=0\n",
    "        else:\n",
    "            jmin=j+10\n",
    "        if j+10>len(nf)-1:\n",
    "            jmax=len(nf-1)\n",
    "        else:\n",
    "            jmax=j+10\n",
    "            \n",
    "        max_nf=nf[jmin]\n",
    "        min_nf=nf[jmin]\n",
    "        for k in range(jmin,jmax):\n",
    "            if nf[k] >= max_nf:\n",
    "                max_nf = nf[k]\n",
    "            elif nf[k] <= min_nf:\n",
    "                min_nf = nf[k]\n",
    "        \n",
    "        if nf[j]==max_nf:\n",
    "            naivemax.append(nf[j])\n",
    "            nmax_times.append(t[j])\n",
    "            nmax_inds.append(j)\n",
    "        elif nf[j]==min_nf:\n",
    "            naivemins.append(nf[j])\n",
    "            nmin_times.append(t[j])\n",
    "            nmin_inds.append(j)\n",
    "            \n",
    "    naivemax = np.array(naivemax)\n",
    "    nmax_times = np.array(nmax_times)\n",
    "    nmax_inds = np.array(nmax_inds)\n",
    "    naivemins = np.array(naivemins)\n",
    "    nmin_times = np.array(nmin_times)\n",
    "    nmin_inds = np.array(nmin_inds)\n",
    "    \n",
    "    return naivemax,nmax_times,nmax_inds,naivemins,nmin_times,nmin_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.9 µs ± 717 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit naivemax,nmax_times,nmax_inds,naivemins,nmin_times,nmin_inds=n_mins_maxes(t.astype(np.float64),nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_mins_maxes(t,nf):\n",
    "    naivemax,nmax_times = [],[]\n",
    "    naivemins,nmin_times = [],[]\n",
    "    for j in range(len(nf)):\n",
    "        if nf[j] == max(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "            naivemax.append(nf[j])\n",
    "            nmax_times.append(t[j])\n",
    "        elif nf[j] == min(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "            naivemins.append(nf[j])\n",
    "            nmin_times.append(t[j])\n",
    "            \n",
    "    naivemax = np.array(naivemax)\n",
    "    nmax_times = np.array(nmax_times)\n",
    "    nmax_inds = np.argsort(t)[np.searchsorted(t,nmax_times,sorter=np.argsort(t))]\n",
    "    naivemins = np.array(naivemins)\n",
    "    nmin_times = np.array(nmin_times)\n",
    "    nmin_inds = np.argsort(t)[np.searchsorted(t,nmin_times,sorter=np.argsort(t))]\n",
    "    \n",
    "    return naivemax,nmax_times,nmax_inds,naivemins,nmin_times,nmin_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit naivemax,nmax_times,nmax_inds,naivemins,nmin_times,nmin_inds=n_mins_maxes(t.astype(np.float64),nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_mins_maxes(nf):\n",
    "    naivemax,nmax_times = [],[]\n",
    "    naivemins,nmin_times = [],[]\n",
    "    for j in range(len(nf)):\n",
    "        if nf[j] == max(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "            naivemax.append(nf[j])\n",
    "            nmax_times.append(t[j])\n",
    "        elif nf[j] == min(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "            naivemins.append(nf[j])\n",
    "            nmin_times.append(t[j])\n",
    "            \n",
    "    naivemax = np.array(naivemax)\n",
    "    nmax_times = np.array(nmax_times)\n",
    "    nmax_inds = np.argsort(t)[np.searchsorted(t,nmax_times,sorter=np.argsort(t))]\n",
    "    naivemins = np.array(naivemins)\n",
    "    nmin_times = np.array(nmin_times)\n",
    "    nmin_inds = np.argsort(t)[np.searchsorted(t,nmin_times,sorter=np.argsort(t))]\n",
    "    \n",
    "    return naivemax,nmax_times,nmax_inds,naivemins,nmin_times,nmin_inds\n",
    "\n",
    "@njit\n",
    "def easy_feats(t,nf,err):\n",
    "    nf_mean = np.mean(nf)\n",
    "    nf_med = np.median(nf)\n",
    "    stds = np.std(nf) #f6\n",
    "    meanmedrat = nf_mean / nf_med # F2\n",
    "    varss = np.var(nf) # F4\n",
    "    coeffvar = stds/nf_mean #F5\n",
    "\n",
    "    posthreshold = nf_mean+4*stds\n",
    "    negthreshold = nf_mean-4*stds\n",
    "\n",
    "    numout1s = len(nf[np.abs(nf-nf_mean)>stds])\n",
    "    numposoutliers = len(nf[nf>posthreshold])\n",
    "    numnegoutliers = len(nf[nf<negthreshold])\n",
    "\n",
    "    numoutliers=numposoutliers+numnegoutliers #F10\n",
    "\n",
    "    mad = np.median(np.abs(nf-nf_med))\n",
    "\n",
    "    # delta nf/delta t\n",
    "    slopes = (nf[1:]-nf[:-1])/(t[1:]-t[:-1])\n",
    "    meanslope = np.mean(slopes) #F12\n",
    "\n",
    "    # Separating positive slopes and negative slopes\n",
    "    # Should both include the 0 slope? It doesn't matter for calculating the means later on...\n",
    "    pslope = slopes[slopes>=0]\n",
    "    nslope = slopes[slopes<=0]\n",
    "    # Looking at the average (mean) positive and negative slopes\n",
    "    if len(pslope)==0:meanpslope=0\n",
    "    else:meanpslope=np.mean(pslope) #F15\n",
    "\n",
    "    if len(nslope)==0:meannslope=0\n",
    "    else:meannslope=np.mean(nslope) #F16\n",
    "\n",
    "    # Quantifying the difference in shape.\n",
    "    # if meannslope==0 (i.e., if there are no negative slopes), g_asymm is assigned a value of 10\n",
    "    # This value is chosen such that \n",
    "    # a) it is positive (where g_asymm is inherently negative), \n",
    "    # b) it is a factor larger than a random signal would produce (roughly equal average of positive and negative slopes -> g_asymm=-1)\n",
    "    # c) it is not orders of magnitude larger than other data, which would affect outlier analysis\n",
    "    if meannslope==0:g_asymm = 10\n",
    "    else:g_asymm=meanpslope / meannslope #F17\n",
    "\n",
    "    # Won't this be skewed by the fact that both pslope and nslope have all the 0's? Eh\n",
    "    if len(nslope)==0:rough_g_asymm=10\n",
    "    else:rough_g_asymm=len(pslope) / len(nslope) #F18\n",
    "\n",
    "    # meannslope is inherently negative, so this is the difference btw the 2\n",
    "    diff_asymm=meanpslope + meannslope #F19\n",
    "    \n",
    "    absslopes = np.abs(slopes)\n",
    "    meanabsslope=np.mean(absslopes) #F21\n",
    "    varabsslope=np.var(absslopes) #F22\n",
    "    varslope=np.var(slopes) #F23\n",
    "\n",
    "    # secder = Second Derivative\n",
    "    # Reminder for self: the slope is \"located\" halfway between the flux and time points, \n",
    "    # so the delta t in the denominator is accounting for that.\n",
    "    # secder = delta slopes/delta t, delta t = ((t_j-t_(j-1))+(t_(j+1)-t_j))/2\n",
    "    # secder=[(slopes[j]-slopes[j-1])/((t[j+1]-t[j])/2+(t[j]-t[j-1])/2) for j in range(1, len(slopes)-1)]\n",
    "    # after algebraic simplification:\n",
    "    secder = 2*(slopes[1:]-slopes[:-1])/(t[1:-1]-t[:-2])\n",
    "\n",
    "    # abssecder=[abs((slopes[j]-slopes[j-1])/((t[j+1]-t[j])/2+(t[j]-t[j-1])/2)) for j in range (1, len(slopes)-1)]\n",
    "    # simplification:\n",
    "\n",
    "    abssecder=np.abs(secder)\n",
    "    absmeansecder=np.mean(abssecder) #F24\n",
    "\n",
    "    if len(pslope)==0:pslopestds=0\n",
    "    else:pslopestds=np.std(pslope)\n",
    "\n",
    "    if len(nslope)==0:\n",
    "        nslopesstds=0\n",
    "        stdratio=10\n",
    "    else:\n",
    "        nslopestds=np.std(nslope)\n",
    "        stdratio=pslopestds/nslopestds\n",
    "\n",
    "    sdstds=np.std(secder)\n",
    "    meanstds=np.mean(secder)\n",
    "\n",
    "    num_pspikes=len(slopes[slopes>=meanpslope+3*pslopestds]) #F25\n",
    "    num_nspikes=len(slopes[slopes<=meannslope-3*nslopestds]) #F26\n",
    "\n",
    "    # 5/30/18, discovered a typo here. meanslope was missing an 'n', i.e. all data\n",
    "    # processed prior to this date has num_nspikes defined as meanslope-3*nslopestds\n",
    "    # which will overestimate the number of negative spikes since meanslope is inherently\n",
    "    # greater than meannslope.\n",
    "\n",
    "    num_psdspikes = len(secder[secder>=4*sdstds]) #F27\n",
    "    num_nsdspikes = len(secder[secder<=4*sdstds]) #F28\n",
    "    if nslopestds==0:\n",
    "        stdratio=10\n",
    "    else:\n",
    "        stdratio = pslopestds / nslopestds #F29\n",
    "\n",
    "    # The ratio of postive slopes with a following postive slope to the total number of points.\n",
    "    pairs = np.where((slopes[1:]>0)&(slopes[:-1]>0))[0] # where positive slopes are followed by another positive slope\n",
    "    pstrend=len(pairs)/len(slopes) #F30\n",
    "\n",
    "    # Checks if the flux crosses the 'zero' line.\n",
    "    zcrossind = np.where(corrnf[:-1]*corrnf[1:]<0)\n",
    "    num_zcross = len(zcrossind) #F31\n",
    "\n",
    "    plusminus = np.where((slopes[1:]<0)&(slopes[:-1]>0))[0]\n",
    "    num_pm = len(plusminus)\n",
    "\n",
    "    # This looks up the local maximums. Adds a peak if it's the largest within 10 points on either side.\n",
    "    # Q: Is there a way to do this and take into account drastically different periodicity scales?\n",
    "\n",
    "    naivemax,nmax_times,nmax_inds = [],[],[]\n",
    "    naivemins,nmin_times,nmin_inds = [],[],[]\n",
    "    for j in range(len(nf)):\n",
    "        nfj = nf[j]\n",
    "        if j-10<0:\n",
    "            jmin=0\n",
    "        else:\n",
    "            jmin=j+10\n",
    "        if j+10>len(nf)-1:\n",
    "            jmax=len(nf-1)\n",
    "        else:\n",
    "            jmax=j+10\n",
    "            \n",
    "        max_nf=nf[jmin]\n",
    "        min_nf=nf[jmin]\n",
    "        for k in range(jmin,jmax):\n",
    "            if nf[k] >= max_nf:\n",
    "                max_nf = nf[k]\n",
    "            elif nf[k] <= min_nf:\n",
    "                min_nf = nf[k]\n",
    "        \n",
    "        if nf[j]==max_nf:\n",
    "            naivemax.append(nf[j])\n",
    "            nmax_times.append(t[j])\n",
    "            nmax_inds.append(j)\n",
    "        elif nf[j]==min_nf:\n",
    "            naivemins.append(nf[j])\n",
    "            nmin_times.append(t[j])\n",
    "            nmin_inds.append(j)\n",
    "            \n",
    "    naivemax = np.array(naivemax)\n",
    "    nmax_times = np.array(nmax_times)\n",
    "    nmax_inds = np.array(nmax_inds)\n",
    "    naivemins = np.array(naivemins)\n",
    "    nmin_times = np.array(nmin_times)\n",
    "    nmin_inds = np.array(nmin_inds)\n",
    "    \n",
    "    len_nmax=len(naivemax) #F33\n",
    "    len_nmin=len(naivemins) #F34\n",
    "\n",
    "    ppslopes = (naivemax[1:]-naivemax[:-1])/(nmax_times[1:]-nmax_times[:-1])\n",
    "\n",
    "    if len(ppslopes)==0:\n",
    "        ptpslopes = 0\n",
    "    else:\n",
    "        ptpslopes=np.mean(ppslopes) #F36\n",
    "\n",
    "    maxdiff = nmax_times[1:]-nmax_times[:-1]\n",
    "    \n",
    "    \n",
    "    emin = naivemins[::2] # even indice minimums\n",
    "    omin = naivemins[1::2] # odd indice minimums\n",
    "    meanemin = np.mean(emin)\n",
    "    if len(omin)==0:\n",
    "        meanomin=0\n",
    "    else:\n",
    "        meanomin = np.mean(omin)\n",
    "    oeratio = meanomin/meanemin #F42\n",
    "\n",
    "    #measures the slope before and after the maximums\n",
    "    # reminder: 1 less slope than flux, slopes start after first flux\n",
    "    # slope[0] is between flux[0] and flux[1]\n",
    "        # mean of slopes before max will be positive\n",
    "        # mean of slopes after max will be negative\n",
    "\n",
    "    nmax_inds_subset = nmax_inds[(nmax_inds>5)&(nmax_inds<len(slopes)-5)]\n",
    "    flatness = np.zeros(len(nmax_inds_subset))\n",
    "    for i,j in enumerate(nmax_inds_subset):\n",
    "        flatness[i] = np.mean(slopes[j-6:j-1])-np.mean(slopes[j:j+5])\n",
    "\n",
    "    if len(flatness)==0: flatmean=0\n",
    "    else: flatmean = np.mean(flatness) #F55\n",
    "\n",
    "    # measures the slope before and after the minimums\n",
    "    # trying flatness w slopes and nf rather than \"corr\" vals, despite orig def in RN's program\n",
    "      # mean of slopes before min will be negative\n",
    "      # mean of slopes after min will be positive\n",
    "\n",
    "\n",
    "    nmin_inds_subset = nmin_inds[(nmin_inds>5)&(nmin_inds<len(slopes)-5)]\n",
    "    tflatness = np.zeros(len(nmin_inds_subset))\n",
    "    for i,j in enumerate(nmin_inds_subset):\n",
    "        tflatness[i] = -np.mean(slopes[j-6:j-1])+np.mean(slopes[j:j+5])\n",
    "\n",
    "    # tflatness for mins, flatness for maxes\n",
    "    if len(tflatness)==0: tflatmean=0\n",
    "    else: tflatmean = np.mean(tflatness) #F56\n",
    "\n",
    "    # reminder: 1 less second derivative than slope (2 less than flux). secder starts after first slope.\n",
    "    # secder[0] is between slope[0] and slope[1], centered at flux[1]\n",
    "\n",
    "    nmax_inds_subset = nmax_inds[(nmax_inds>5)&(nmax_inds<len(secder-5))]\n",
    "    \n",
    "    roundness = np.zeros(len(nmax_inds_subset))\n",
    "    for i,j in enumerate(nmax_inds_subset):\n",
    "        roundness[i] = np.mean(secder[j-6:j+6])*2\n",
    "        \n",
    "    if len(roundness)==0: roundmean=0\n",
    "    else: roundmean = np.mean(roundness) #F57\n",
    "\n",
    "    nmin_inds_subset = nmin_inds[(nmin_inds>5)&(nmin_inds<len(secder-5))]\n",
    "    troundness = np.zeros(len(nmin_inds_subset))\n",
    "    for i,j in enumerate(nmin_inds_subset):\n",
    "        troundness[i] = np.mean(secder[j-6:j+6])*2\n",
    "\n",
    "    if len(troundness)==0: troundmean=0\n",
    "    else: troundmean = np.mean(troundness) #F58\n",
    "\n",
    "    if troundmean==0 and roundmean==0: roundrat=1\n",
    "    elif troundmean==0: roundrat=10\n",
    "    else: roundrat = roundmean / troundmean #F59\n",
    "\n",
    "    if flatmean==0 and tflatmean==0: flatrat=1\n",
    "    elif tflatmean==0: flatrat=10\n",
    "    else: flatrat = flatmean / tflatmean #F60\"\"\"\n",
    "        \n",
    "    return meanmedrat, skews, varss, coeffvar, stds, numoutliers, numnegoutliers, numposoutliers, numout1s, mad, meanpslope, meannslope, g_asymm, rough_g_asymm, diff_asymm, varabsslope, varslope, meanabsslope, absmeansecder, num_pspikes, num_nspikes, num_psdspikes, num_nsdspikes,stdratio, pstrend, num_zcross, num_pm, len_nmax, len_nmin, ptpslopes, oeratio, flatmean, tflatmean, roundmean, troundmean, roundrat, flatrat\n",
    "\n",
    "def fancy_feats(t,nf,err):\n",
    "    # fancy meaning I can't throw these under a jit decorator.\n",
    "    longtermtrend = np.polyfit(t, nf, 1)[0] # Feature 1 (Abbr. F1) overall slope\n",
    "    yoff = np.polyfit(t, nf, 1)[1] # Not a feature, y-intercept of linear fit\n",
    "    skews = stats.skew(nf) # F3\n",
    "    corrnf = nf - longtermtrend*t - yoff #this removes any linear trend of lc so you can look at just troughs\n",
    "    \n",
    "    kurt = stats.kurtosis(nf)\n",
    "    \n",
    "    # by looking at where the 99th percentile is instead of just the largest number,\n",
    "    # I think it avoids the extremes which might not be relevant (might be unreliable data)\n",
    "    # Is the miniumum slope the most negative one, or the flattest one? Answer: Most negative\n",
    "    maxslope=np.percentile(slopes,99) #F13\n",
    "    minslope=np.percentile(slopes,1) #F14\n",
    "\n",
    "    #corrslopes (corrected slopes) removes the longterm linear trend (if any) and then looks at the slope\n",
    "    corrslopes = (corrnf[1:]-corrnf[:-1])/(t[1:]-t[:-1])\n",
    "    skewslope = stats.skew(slopes) #F20\n",
    "\n",
    "    if len(naivemax)>2:\n",
    "        mautocorrcoef = np.corrcoef(naivemax[:-1], naivemax[1:])[0][1] #F35\n",
    "    else:\n",
    "        mautocorrcoef = 0\n",
    "\n",
    "    if len(maxdiff)==0:\n",
    "        periodicity=0\n",
    "        periodicityr=0\n",
    "        naiveperiod=0\n",
    "    else:\n",
    "        periodicity=np.std(maxdiff)/np.mean(maxdiff) #F37\n",
    "        periodicityr=np.sum(abs(maxdiff-np.mean(maxdiff)))/np.mean(maxdiff) #F38\n",
    "        naiveperiod=np.mean(maxdiff) #F39\n",
    "    if len(naivemax)==0:\n",
    "        maxvars=0\n",
    "        maxvarsr=0\n",
    "    else:\n",
    "        maxvars = np.std(naivemax)/np.mean(naivemax) #F40\n",
    "        maxvarsr = np.sum(abs(naivemax-np.mean(naivemax)))/np.mean(naivemax) #F41\n",
    "\n",
    "    # amp here is actually amp_2 in revantese\n",
    "    # 2x the amplitude (peak-to-peak really)\n",
    "    amp = np.percentile(nf,99)-np.percentile(nf,1) #F43\n",
    "    normamp = amp / nf_mean #this should prob go, since flux is norm'd #F44\n",
    "    \n",
    "    autocorrcoef = np.corrcoef(nf[:-1], nf[1:])[0][1] #F54\n",
    "\n",
    "    sautocorrcoef = np.corrcoef(slopes[:-1], slopes[1:])[0][1] #F55\n",
    "    # ratio of points within one fifth of the amplitude to the median to total number of points \n",
    "    mbp = len(nf[(nf<=(nf_med+0.1*amp))&(nf>=(nf_med-0.1*amp))]) / len(nf) #F45\n",
    "\n",
    "    f595 = np.percentile(nf,95)-np.percentile(nf,5)\n",
    "    f1090 =np.percentile(nf,90)-np.percentile(nf,10)\n",
    "    f1782 =np.percentile(nf, 82)-np.percentile(nf, 17)\n",
    "    f2575 =np.percentile(nf, 75)-np.percentile(nf, 25)\n",
    "    f3267 =np.percentile(nf, 67)-np.percentile(nf, 32)\n",
    "    f4060 =np.percentile(nf, 60)-np.percentile(nf, 40)\n",
    "    mid20 =f4060/f595 #F46\n",
    "    mid35 =f3267/f595 #F47\n",
    "    mid50 =f2575/f595 #F48\n",
    "    mid65 =f1782/f595 #F49\n",
    "    mid80 =f1090/f595 #F50 \n",
    "\n",
    "    \n",
    "    percentamp = max(np.abs(nf-nf_med)/nf_med) #F51\n",
    "\n",
    "    magratio = (max(nf)-nf_med) / amp #F52\n",
    "    \n",
    "    return longtermtrend, kurt, maxslope, minslope, skewslope, mautocorrcoef,periodicity,periodicityr,naiveperiod,maxvars,maxvarsr,amp,normamp,mbp,mid20,mid35,mid50,mid65,mid80,percentamp,magratio,sautocorrcoef,autocorrcoef\n",
    "    \n",
    "def feature_calc(t,nf,err):\n",
    "    meanmedrat, skews, varss, coeffvar, stds, numoutliers, numnegoutliers, numposoutliers, numout1s, mad, meanpslope, meannslope, g_asymm, rough_g_asymm, diff_asymm, varabsslope, varslope, meanabsslope, absmeansecder, num_pspikes, num_nspikes, num_psdspikes, num_nsdspikes,stdratio, pstrend, num_zcross, num_pm, len_nmax, len_nmin, ptpslopes, oeratio, flatmean, tflatmean, roundmean, troundmean, roundrat, flatrat=easy_feats(t,nf,err)\n",
    "    longtermtrend, kurt, maxslope, minslope, skewslope, mautocorrcoef,periodicity,periodicityr,naiveperiod,maxvars,maxvarsr,amp,normamp,mbp,mid20,mid35,mid50,mid65,mid80,percentamp,magratio,sautocorrcoef,autocorrcoef=fancy_feats(t,nf,err)\n",
    "    ndata = [longtermtrend, meanmedrat, skews, varss, coeffvar, stds, \\\n",
    "                 numoutliers, numnegoutliers, numposoutliers, numout1s, kurt, mad, \\\n",
    "                 maxslope, minslope, meanpslope, meannslope, g_asymm, rough_g_asymm, \\\n",
    "                 diff_asymm, skewslope, varabsslope, varslope, meanabsslope, absmeansecder, \\\n",
    "                 num_pspikes, num_nspikes, num_psdspikes, num_nsdspikes,stdratio, pstrend, \\\n",
    "                 num_zcross, num_pm, len_nmax, len_nmin, mautocorrcoef, ptpslopes, \\\n",
    "                 periodicity, periodicityr, naiveperiod, maxvars, maxvarsr, oeratio, \\\n",
    "                 amp, normamp, mbp, mid20, mid35, mid50, \\\n",
    "                 mid65, mid80, percentamp, magratio, sautocorrcoef, autocorrcoef, \\\n",
    "                 flatmean, tflatmean, roundmean, troundmean, roundrat, flatrat]\n",
    "    return ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def ffunc(a,b):\n",
    "    c = np.mean(a)+np.mean(b)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffunc(np.array([1,2,3]),np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 ms ± 72.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit feature_calc(t,nf,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31 ms ± 222 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "feature_calc(t.astype(np.float64),nf,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 µs ± 51.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit feature_calc.py_func(t.astype(np.float64),nf,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_calc(t,nf,err):\n",
    "    longtermtrend = np.polyfit(t, nf, 1)[0] # Feature 1 (Abbr. F1) overall slope\n",
    "    yoff = np.polyfit(t, nf, 1)[1] # Not a feature, y-intercept of linear fit\n",
    "    nf_mean = np.mean(nf)\n",
    "    nf_med = np.median(nf)\n",
    "    stds = np.std(nf) #f6\n",
    "    meanmedrat = nf_mean / nf_med # F2\n",
    "    skews = stats.skew(nf) # F3\n",
    "    varss = np.var(nf) # F4\n",
    "    coeffvar = stds/nf_mean #F5\n",
    "\n",
    "\n",
    "    corrnf = nf - longtermtrend*t - yoff #this removes any linear trend of lc so you can look at just troughs\n",
    "\n",
    "    posthreshold = nf_mean+4*stds\n",
    "    negthreshold = nf_mean-4*stds\n",
    "\n",
    "    numout1s = len(nf[np.abs(nf-nf_mean)>stds])\n",
    "    numposoutliers = len(nf[nf>posthreshold])\n",
    "    numnegoutliers = len(nf[nf<negthreshold])\n",
    "\n",
    "    numoutliers=numposoutliers+numnegoutliers #F10\n",
    "    \n",
    "    kurt = stats.kurtosis(nf)\n",
    "\n",
    "    mad = np.median(np.abs(nf-nf_med))\n",
    "\n",
    "    # delta nf/delta t\n",
    "    slopes = (nf[1:]-nf[:-1])/(t[1:]-t[:-1])\n",
    "\n",
    "    #corrslopes (corrected slopes) removes the longterm linear trend (if any) and then looks at the slope\n",
    "    corrslopes = (corrnf[1:]-corrnf[:-1])/(t[1:]-t[:-1])\n",
    "    meanslope = np.mean(slopes) #F12\n",
    "\n",
    "    # by looking at where the 99th percentile is instead of just the largest number,\n",
    "    # I think it avoids the extremes which might not be relevant (might be unreliable data)\n",
    "    # Is the miniumum slope the most negative one, or the flattest one? Answer: Most negative\n",
    "    maxslope=np.percentile(slopes,99) #F13\n",
    "    minslope=np.percentile(slopes,1) #F14\n",
    "\n",
    "    # Separating positive slopes and negative slopes\n",
    "    # Should both include the 0 slope? It doesn't matter for calculating the means later on...\n",
    "    pslope = slopes[slopes>=0]\n",
    "    nslope = slopes[slopes<=0]\n",
    "    # Looking at the average (mean) positive and negative slopes\n",
    "    if len(pslope)==0:meanpslope=0\n",
    "    else:meanpslope=np.mean(pslope) #F15\n",
    "\n",
    "    if len(nslope)==0:meannslope=0\n",
    "    else:meannslope=np.mean(nslope) #F16\n",
    "\n",
    "    # Quantifying the difference in shape.\n",
    "    # if meannslope==0 (i.e., if there are no negative slopes), g_asymm is assigned a value of 10\n",
    "    # This value is chosen such that \n",
    "    # a) it is positive (where g_asymm is inherently negative), \n",
    "    # b) it is a factor larger than a random signal would produce (roughly equal average of positive and negative slopes -> g_asymm=-1)\n",
    "    # c) it is not orders of magnitude larger than other data, which would affect outlier analysis\n",
    "    if meannslope==0:g_asymm = 10\n",
    "    else:g_asymm=meanpslope / meannslope #F17\n",
    "\n",
    "    # Won't this be skewed by the fact that both pslope and nslope have all the 0's? Eh\n",
    "    if len(nslope)==0:rough_g_asymm=10\n",
    "    else:rough_g_asymm=len(pslope) / len(nslope) #F18\n",
    "\n",
    "    # meannslope is inherently negative, so this is the difference btw the 2\n",
    "    diff_asymm=meanpslope + meannslope #F19\n",
    "    skewslope = stats.skew(slopes) #F20\n",
    "    absslopes = np.abs(slopes)\n",
    "    meanabsslope=np.mean(absslopes) #F21\n",
    "    varabsslope=np.var(absslopes) #F22\n",
    "    varslope=np.var(slopes) #F23\n",
    "\n",
    "    # secder = Second Derivative\n",
    "    # Reminder for self: the slope is \"located\" halfway between the flux and time points, \n",
    "    # so the delta t in the denominator is accounting for that.\n",
    "    # secder = delta slopes/delta t, delta t = ((t_j-t_(j-1))+(t_(j+1)-t_j))/2\n",
    "    # secder=[(slopes[j]-slopes[j-1])/((t[j+1]-t[j])/2+(t[j]-t[j-1])/2) for j in range(1, len(slopes)-1)]\n",
    "    # after algebraic simplification:\n",
    "    secder = 2*(slopes[1:]-slopes[:-1])/(t[1:-1]-t[:-2])\n",
    "\n",
    "    # abssecder=[abs((slopes[j]-slopes[j-1])/((t[j+1]-t[j])/2+(t[j]-t[j-1])/2)) for j in range (1, len(slopes)-1)]\n",
    "    # simplification:\n",
    "\n",
    "    abssecder=np.abs(secder)\n",
    "    absmeansecder=np.mean(abssecder) #F24\n",
    "\n",
    "    if len(pslope)==0:pslopestds=0\n",
    "    else:pslopestds=np.std(pslope)\n",
    "\n",
    "    if len(nslope)==0:\n",
    "        nslopesstds=0\n",
    "        stdratio=10\n",
    "    else:\n",
    "        nslopestds=np.std(nslope)\n",
    "        stdratio=pslopestds/nslopestds\n",
    "\n",
    "    sdstds=np.std(secder)\n",
    "    meanstds=np.mean(secder)\n",
    "\n",
    "    num_pspikes=len(slopes[slopes>=meanpslope+3*pslopestds]) #F25\n",
    "    num_nspikes=len(slopes[slopes<=meannslope-3*nslopestds]) #F26\n",
    "\n",
    "    # 5/30/18, discovered a typo here. meanslope was missing an 'n', i.e. all data\n",
    "    # processed prior to this date has num_nspikes defined as meanslope-3*nslopestds\n",
    "    # which will overestimate the number of negative spikes since meanslope is inherently\n",
    "    # greater than meannslope.\n",
    "\n",
    "    num_psdspikes = len(secder[secder>=4*sdstds]) #F27\n",
    "    num_nsdspikes = len(secder[secder<=4*sdstds]) #F28\n",
    "    if nslopestds==0:\n",
    "        stdratio=10\n",
    "    else:\n",
    "        stdratio = pslopestds / nslopestds #F29\n",
    "\n",
    "    # The ratio of postive slopes with a following postive slope to the total number of points.\n",
    "    pairs = np.where((slopes[1:]>0)&(slopes[:-1]>0))[0] # where positive slopes are followed by another positive slope\n",
    "    pstrend=len(pairs)/len(slopes) #F30\n",
    "\n",
    "    # Checks if the flux crosses the 'zero' line.\n",
    "    zcrossind = np.where(corrnf[:-1]*corrnf[1:]<0)\n",
    "    num_zcross = len(zcrossind) #F31\n",
    "\n",
    "    plusminus = np.where((slopes[1:]<0)&(slopes[:-1]>0))[0]\n",
    "    num_pm = len(plusminus)\n",
    "\n",
    "    # This looks up the local maximums. Adds a peak if it's the largest within 10 points on either side.\n",
    "    # Q: Is there a way to do this and take into account drastically different periodicity scales?\n",
    "\n",
    "    naivemax,nmax_times = [],[]\n",
    "    naivemins,nmin_times = [],[]\n",
    "    for j in range(len(nf)):\n",
    "        if nf[j] == max(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "            naivemax.append(nf[j])\n",
    "            nmax_times.append(t[j])\n",
    "        elif nf[j] == min(nf[max(j-10,0):min(j+10,len(nf)-1)]):\n",
    "            naivemins.append(nf[j])\n",
    "            nmin_times.append(t[j])\n",
    "    naivemax = np.array(naivemax)\n",
    "    nmax_times = np.array(nmax_times)\n",
    "    nmax_inds = np.argsort(t)[np.searchsorted(t,nmax_times,sorter=np.argsort(t))]\n",
    "    naivemins = np.array(naivemins)\n",
    "    nmin_times = np.array(nmin_times)\n",
    "    nmin_inds = np.argsort(t)[np.searchsorted(t,nmin_times,sorter=np.argsort(t))]\n",
    "\n",
    "\n",
    "    len_nmax=len(naivemax) #F33\n",
    "    len_nmin=len(naivemins) #F34\n",
    "\n",
    "\n",
    "\n",
    "    if len(naivemax)>2:\n",
    "        mautocorrcoef = np.corrcoef(naivemax[:-1], naivemax[1:])[0][1] #F35\n",
    "    else:\n",
    "        mautocorrcoef = 0\n",
    "\n",
    "    \"\"\"peak to peak slopes\"\"\"\n",
    "    ppslopes = (naivemax[1:]-naivemax[:-1])/(nmax_times[1:]-nmax_times[:-1])\n",
    "\n",
    "    if len(ppslopes)==0:\n",
    "        ptpslopes = 0\n",
    "    else:\n",
    "        ptpslopes=np.mean(ppslopes) #F36\n",
    "\n",
    "    maxdiff = nmax_times[1:]-nmax_times[:-1]\n",
    "\n",
    "    if len(maxdiff)==0:\n",
    "        periodicity=0\n",
    "        periodicityr=0\n",
    "        naiveperiod=0\n",
    "    else:\n",
    "        periodicity=np.std(maxdiff)/np.mean(maxdiff) #F37\n",
    "        periodicityr=np.sum(abs(maxdiff-np.mean(maxdiff)))/np.mean(maxdiff) #F38\n",
    "        naiveperiod=np.mean(maxdiff) #F39\n",
    "    if len(naivemax)==0:\n",
    "        maxvars=0\n",
    "        maxvarsr=0\n",
    "    else:\n",
    "        maxvars = np.std(naivemax)/np.mean(naivemax) #F40\n",
    "        maxvarsr = np.sum(abs(naivemax-np.mean(naivemax)))/np.mean(naivemax) #F41\n",
    "\n",
    "    emin = naivemins[::2] # even indice minimums\n",
    "    omin = naivemins[1::2] # odd indice minimums\n",
    "    meanemin = np.mean(emin)\n",
    "    if len(omin)==0:\n",
    "        meanomin=0\n",
    "    else:\n",
    "        meanomin = np.mean(omin)\n",
    "    oeratio = meanomin/meanemin #F42\n",
    "\n",
    "    # amp here is actually amp_2 in revantese\n",
    "    # 2x the amplitude (peak-to-peak really)\n",
    "    amp = np.percentile(nf,99)-np.percentile(nf,1) #F43\n",
    "    normamp = amp / nf_mean #this should prob go, since flux is norm'd #F44\n",
    "\n",
    "    # ratio of points within one fifth of the amplitude to the median to total number of points \n",
    "    mbp = len(nf[(nf<=(nf_med+0.1*amp))&(nf>=(nf_med-0.1*amp))]) / len(nf) #F45\n",
    "\n",
    "    f595 = np.percentile(nf,95)-np.percentile(nf,5)\n",
    "    f1090 =np.percentile(nf,90)-np.percentile(nf,10)\n",
    "    f1782 =np.percentile(nf, 82)-np.percentile(nf, 17)\n",
    "    f2575 =np.percentile(nf, 75)-np.percentile(nf, 25)\n",
    "    f3267 =np.percentile(nf, 67)-np.percentile(nf, 32)\n",
    "    f4060 =np.percentile(nf, 60)-np.percentile(nf, 40)\n",
    "    mid20 =f4060/f595 #F46\n",
    "    mid35 =f3267/f595 #F47\n",
    "    mid50 =f2575/f595 #F48\n",
    "    mid65 =f1782/f595 #F49\n",
    "    mid80 =f1090/f595 #F50 \n",
    "\n",
    "    percentamp = max(np.abs(nf-nf_med)/nf_med) #F51\n",
    "\n",
    "    magratio = (max(nf)-nf_med) / amp #F52\n",
    "\n",
    "    autocorrcoef = np.corrcoef(nf[:-1], nf[1:])[0][1] #F54\n",
    "\n",
    "    sautocorrcoef = np.corrcoef(slopes[:-1], slopes[1:])[0][1] #F55\n",
    "\n",
    "    #measures the slope before and after the maximums\n",
    "    # reminder: 1 less slope than flux, slopes start after first flux\n",
    "    # slope[0] is between flux[0] and flux[1]\n",
    "        # mean of slopes before max will be positive\n",
    "        # mean of slopes after max will be negative\n",
    "\n",
    "    nmax_inds_subset = nmax_inds[(nmax_inds>5)&(nmax_inds<len(slopes)-5)]\n",
    "    flatness = flat_func(slopes,nmax_inds_subset)\n",
    "\n",
    "    if len(flatness)==0: flatmean=0\n",
    "    else: flatmean = np.mean(flatness) #F55\n",
    "\n",
    "    # measures the slope before and after the minimums\n",
    "    # trying flatness w slopes and nf rather than \"corr\" vals, despite orig def in RN's program\n",
    "      # mean of slopes before min will be negative\n",
    "      # mean of slopes after min will be positive\n",
    "\n",
    "\n",
    "    nmin_inds_subset = nmin_inds[(nmin_inds>5)&(nmin_inds<len(slopes)-5)]\n",
    "    tflatness = tflat_func(slopes,nmin_inds_subset)\n",
    "\n",
    "    # tflatness for mins, flatness for maxes\n",
    "    if len(tflatness)==0: tflatmean=0\n",
    "    else: tflatmean = np.mean(tflatness) #F56\n",
    "\n",
    "    # reminder: 1 less second derivative than slope (2 less than flux). secder starts after first slope.\n",
    "    # secder[0] is between slope[0] and slope[1], centered at flux[1]\n",
    "\n",
    "    nmax_inds_subset = nmax_inds[(nmax_inds>5)&(nmax_inds<len(secder-5))]\n",
    "    roundness = roundness_func(secder,nmax_inds_subset)\n",
    "\n",
    "    if len(roundness)==0: roundmean=0\n",
    "    else: roundmean = np.mean(roundness) #F57\n",
    "\n",
    "    nmin_inds_subset = nmin_inds[(nmin_inds>5)&(nmin_inds<len(secder-5))]\n",
    "    troundness = roundness_func(secder,nmin_inds_subset)\n",
    "\n",
    "    if len(troundness)==0: troundmean=0\n",
    "    else: troundmean = np.mean(troundness) #F58\n",
    "\n",
    "    if troundmean==0 and roundmean==0: roundrat=1\n",
    "    elif troundmean==0: roundrat=10\n",
    "    else: roundrat = roundmean / troundmean #F59\n",
    "\n",
    "    if flatmean==0 and tflatmean==0: flatrat=1\n",
    "    elif tflatmean==0: flatrat=10\n",
    "    else: flatrat = flatmean / tflatmean #F60\n",
    "        \n",
    "    ndata = [longtermtrend, meanmedrat, skews, varss, coeffvar, stds, \\\n",
    "                 numoutliers, numnegoutliers, numposoutliers, numout1s, kurt, mad, \\\n",
    "                 maxslope, minslope, meanpslope, meannslope, g_asymm, rough_g_asymm, \\\n",
    "                 diff_asymm, skewslope, varabsslope, varslope, meanabsslope, absmeansecder, \\\n",
    "                 num_pspikes, num_nspikes, num_psdspikes, num_nsdspikes,stdratio, pstrend, \\\n",
    "                 num_zcross, num_pm, len_nmax, len_nmin, mautocorrcoef, ptpslopes, \\\n",
    "                 periodicity, periodicityr, naiveperiod, maxvars, maxvarsr, oeratio, \\\n",
    "                 amp, normamp, mbp, mid20, mid35, mid50, \\\n",
    "                 mid65, mid80, percentamp, magratio, sautocorrcoef, autocorrcoef, \\\n",
    "                 flatmean, tflatmean, roundmean, troundmean, roundrat, flatrat]\n",
    "    return ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3 ms ± 1.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit feature_calc(t,nf,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
